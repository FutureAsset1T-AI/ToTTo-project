{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19355ce7e4fc4a5380484ff42c4d174b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_386612e0533c4beabd247fe8e610c954",
              "IPY_MODEL_9b39dd76c55c4869bf0ccd5df9d9e121",
              "IPY_MODEL_dd2ca803bf7347f28b7376a49ea9ab30"
            ],
            "layout": "IPY_MODEL_5e7999f2d8ee4b3b8f22561fed82c494"
          }
        },
        "386612e0533c4beabd247fe8e610c954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_809899dbed704b8583a27baef222ab46",
            "placeholder": "​",
            "style": "IPY_MODEL_11913da212794e118d99fd16bf8085dc",
            "value": "Map (num_proc=8): 100%"
          }
        },
        "9b39dd76c55c4869bf0ccd5df9d9e121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e55953abdfbe4451b8aa7ba39a5dd707",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4386c12c5c5541aa83638085de5c4a90",
            "value": 50000
          }
        },
        "dd2ca803bf7347f28b7376a49ea9ab30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11f1a2d963b847a4abe1e3fa9af730f5",
            "placeholder": "​",
            "style": "IPY_MODEL_41bdbe56ef624c8184ca6e61e28339fb",
            "value": " 50000/50000 [00:37&lt;00:00, 1264.82 examples/s]"
          }
        },
        "5e7999f2d8ee4b3b8f22561fed82c494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "809899dbed704b8583a27baef222ab46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11913da212794e118d99fd16bf8085dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e55953abdfbe4451b8aa7ba39a5dd707": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4386c12c5c5541aa83638085de5c4a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11f1a2d963b847a4abe1e3fa9af730f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41bdbe56ef624c8184ca6e61e28339fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a0562c9b13544288b4dda339ca0aa30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8d9fbd2bf9d4bc680dc9a770c259d7b",
              "IPY_MODEL_bcda7041db1d49cb828e0c136f3e0571",
              "IPY_MODEL_3fd448796b6843ebb56ee942c7547d5b"
            ],
            "layout": "IPY_MODEL_5ab569f2839f4b6a974883ee64cc29da"
          }
        },
        "c8d9fbd2bf9d4bc680dc9a770c259d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaa8ea305b964beabf0dd2f97c429dac",
            "placeholder": "​",
            "style": "IPY_MODEL_1c7aea324e5341fcba10bdb406ac5068",
            "value": "Map (num_proc=8): 100%"
          }
        },
        "bcda7041db1d49cb828e0c136f3e0571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c643abe3a2fc4be18ec380fe6d7ca563",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62ea8b2d4c4042a78312e54c55b8a87a",
            "value": 1000
          }
        },
        "3fd448796b6843ebb56ee942c7547d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dac936cf25e84dfe87e070e527e974d5",
            "placeholder": "​",
            "style": "IPY_MODEL_630ca0c5c1f3470085167e039ff65755",
            "value": " 1000/1000 [00:05&lt;00:00, 362.83 examples/s]"
          }
        },
        "5ab569f2839f4b6a974883ee64cc29da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaa8ea305b964beabf0dd2f97c429dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c7aea324e5341fcba10bdb406ac5068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c643abe3a2fc4be18ec380fe6d7ca563": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ea8b2d4c4042a78312e54c55b8a87a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dac936cf25e84dfe87e070e527e974d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "630ca0c5c1f3470085167e039ff65755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i41zZ0OdBYuO",
        "outputId": "2c7c7261-f352-4108-84b2-89d259282cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'language'...\n",
            "remote: Enumerating objects: 4053, done.\u001b[K\n",
            "remote: Counting objects: 100% (440/440), done.\u001b[K\n",
            "remote: Compressing objects: 100% (209/209), done.\u001b[K\n",
            "remote: Total 4053 (delta 330), reused 231 (delta 231), pack-reused 3613 (from 4)\u001b[K\n",
            "Receiving objects: 100% (4053/4053), 6.29 MiB | 21.53 MiB/s, done.\n",
            "Resolving deltas: 100% (2304/2304), done.\n",
            "--2026-01-19 14:25:12--  https://storage.googleapis.com/totto-public/totto_data.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.189.207, 192.178.129.207, 142.251.184.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.189.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 187724372 (179M) [application/zip]\n",
            "Saving to: ‘totto_data.zip’\n",
            "\n",
            "totto_data.zip      100%[===================>] 179.03M   284MB/s    in 0.6s    \n",
            "\n",
            "2026-01-19 14:25:13 (284 MB/s) - ‘totto_data.zip’ saved [187724372/187724372]\n",
            "\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from -r eval_requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from -r eval_requirements.txt (line 2)) (8.4.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (from -r eval_requirements.txt (line 3)) (2.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from -r eval_requirements.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from -r eval_requirements.txt (line 5)) (0.45.1)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->-r eval_requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from pytest->-r eval_requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->-r eval_requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->-r eval_requirements.txt (line 2)) (2.19.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu->-r eval_requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu->-r eval_requirements.txt (line 3)) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu->-r eval_requirements.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu->-r eval_requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu->-r eval_requirements.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu->-r eval_requirements.txt (line 3)) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "# 1. 구글 리서치 깃허브 코드 다운로드\n",
        "!git clone https://github.com/google-research/language.git\n",
        "\n",
        "# 2. ToTTo 데이터셋 다운로드 및 압축 풀기\n",
        "!wget https://storage.googleapis.com/totto-public/totto_data.zip\n",
        "!unzip -q totto_data.zip\n",
        "\n",
        "# 3. 필요한 라이브러리 설치\n",
        "import os\n",
        "os.chdir('/content/language/language/totto') # 작업 폴더로 이동\n",
        "!pip install -r eval_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/language/language/totto/totto_parent_eval.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sAy8amb5CVwB",
        "outputId": "5fa25748-70e0-465d-dc2a-5f4c905553b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# coding=utf-8\n",
            "# Copyright 2018 The Google AI Language Team Authors.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "r\"\"\"Script to compute PARENT metric modified for ToTTo dataset.\n",
            "\n",
            "https://arxiv.org/abs/1906.01081\n",
            "\n",
            "Modified from:\n",
            "https://github.com/google-research/language/tree/master/language/table_text_eval\n",
            "\n",
            "\n",
            "The <reference_file> and <generation_file> should contain references and\n",
            "generations, respectively, one per line. The <table_file> should contain the\n",
            "ground truth tables corresponding to these in each line.\n",
            "\n",
            "For multiple references, each reference should be in a separate file and named\n",
            "like: '<reference-path>0', '<reference-path>1',.....\n",
            "\n",
            "The format for tables is pairs of attributes and values:\n",
            "  attribute_1|||value_1<TAB>attribute_2|||value_2<TAB>...\n",
            "\n",
            "The entailment probability is computing using word overlap.\n",
            "\"\"\"\n",
            "import collections\n",
            "import glob\n",
            "import io\n",
            "import math\n",
            "import os\n",
            "\n",
            "from absl import app\n",
            "from absl import flags\n",
            "import sacrebleu\n",
            "\n",
            "try:\n",
            "  import sacrebleu.tokenizers.tokenizer_13a  # pylint: disable=g-import-not-at-top\n",
            "except ImportError:\n",
            "  raise ValueError(\"Latest version of sacrebleu is missing.\")\n",
            "\n",
            "FLAGS = flags.FLAGS\n",
            "\n",
            "flags.DEFINE_string(\n",
            "    \"reference_path\", None, \"Text file containing references, one per line. \"\n",
            "    \"Multiple references should be in different files named as: \"\n",
            "    \"'<reference_path>0', '<reference_path>1', '<reference_path>2'.\")\n",
            "\n",
            "flags.DEFINE_string(\"generation_path\", None,\n",
            "                    \"Text file containing generations, one per line.\")\n",
            "\n",
            "flags.DEFINE_string(\n",
            "    \"precision_table_path\", None,\n",
            "    \"Text file containing tables for computing precision, one per line.\")\n",
            "\n",
            "flags.DEFINE_string(\n",
            "    \"recall_table_path\", None,\n",
            "    \"Text file containing tables for computing recall, one per line.\")\n",
            "\n",
            "flags.DEFINE_float(\"smoothing\", 0.00001,\n",
            "                   \"Constant to replace 0 precision and recall scores with.\")\n",
            "\n",
            "flags.DEFINE_float(\"lambda_weight\", None,\n",
            "                   \"Weighting factor for recall computed against the table.\")\n",
            "\n",
            "\n",
            "def _normalize_text(s):\n",
            "  # pylint: disable=unnecessary-lambda\n",
            "  tokenize_fn = lambda x: sacrebleu.tokenizers.tokenizer_13a.Tokenizer13a()(x)\n",
            "  return tokenize_fn(s.strip().lower())\n",
            "\n",
            "\n",
            "def _text_reader(text_file):\n",
            "  \"\"\"Returns list of lines from the text file.\n",
            "\n",
            "  Performs lowercasing and white-space tokenization on each line before\n",
            "  returning.\n",
            "\n",
            "  Args:\n",
            "    text_file: String filename.\n",
            "  \"\"\"\n",
            "  texts = []\n",
            "  with io.open(text_file, encoding=\"utf-8\") as f:\n",
            "    for line in f:\n",
            "      line = _normalize_text(line)\n",
            "      texts.append(line.split())\n",
            "  return texts\n",
            "\n",
            "\n",
            "def _text_reference_reader(text_file):\n",
            "  \"\"\"Returns list of references in both the single and multi-reference setting.\n",
            "\n",
            "  Performs lowercasing and white-space tokenization on each line before\n",
            "  returning.\n",
            "\n",
            "  Args:\n",
            "    text_file: String filename.\n",
            "  \"\"\"\n",
            "  single_reference_exists = os.path.isfile(text_file)\n",
            "\n",
            "  # Check for multi-references.\n",
            "  multi_reference_paths = glob.glob(text_file + \"[0-9]\")\n",
            "\n",
            "  # Either the file should exist or it should correspond to multiple reference\n",
            "  # files but not both.\n",
            "  assert ((single_reference_exists or multi_reference_paths) and\n",
            "          (not single_reference_exists or not multi_reference_paths))\n",
            "\n",
            "  if single_reference_exists:\n",
            "    references = _text_reader(text_file)\n",
            "    references = [[x] for x in references]\n",
            "    return references\n",
            "  else:\n",
            "    # In the multi-reference case, we should have 3 reference files.\n",
            "    assert len(multi_reference_paths) == 3\n",
            "    references0 = _text_reader(multi_reference_paths[0])\n",
            "    references1 = _text_reader(multi_reference_paths[1])\n",
            "    references2 = _text_reader(multi_reference_paths[2])\n",
            "\n",
            "    assert len(references0) == len(references1)\n",
            "    assert len(references0) == len(references2)\n",
            "    multi_references = []\n",
            "    for i in range(len(references0)):\n",
            "      multi_reference = [references0[i], references1[i], references2[i]]\n",
            "      multi_references.append(multi_reference)\n",
            "\n",
            "    return multi_references\n",
            "\n",
            "\n",
            "def _table_reader(table_file):\n",
            "  \"\"\"Yields tables from the table file.\n",
            "\n",
            "  Tables are parsed into a list of tuples with tokenized entries.\n",
            "\n",
            "  Args:\n",
            "    table_file: String filename.\n",
            "  \"\"\"\n",
            "  with io.open(table_file, encoding=\"utf-8\") as f:\n",
            "    for line in f:\n",
            "      entries = line.lower().split(\"\\t\")\n",
            "      # pylint: disable=g-complex-comprehension\n",
            "      table = [[\n",
            "          _normalize_text(member).split() for member in entry.split(\"|||\")\n",
            "      ] for entry in entries]\n",
            "      yield table\n",
            "\n",
            "\n",
            "def overlap_probability(ngram, table, smoothing=0.0, stopwords=None):\n",
            "  \"\"\"Returns the probability that the given n-gram overlaps with the table.\n",
            "\n",
            "  A simple implementation which checks how many tokens in the n-gram are also\n",
            "  among the values in the table. For tables with (attribute, value) pairs on the\n",
            "  `value` field is condidered. For tables with (head, relation, tail) triples a\n",
            "  concatenation of `head` and `tail` are considered.\n",
            "\n",
            "  E.g.:\n",
            "    >>> overlap_probability([\"michael\", \"dahlquist\"],\n",
            "                             [([\"name\"], [\"michael\", \"dahlquist\"])])\n",
            "    >>> 1.0\n",
            "\n",
            "  Args:\n",
            "    ngram: List of tokens.\n",
            "    table: List of either (attribute, value) pairs or (head, relation, tail)\n",
            "      triples. Each member of the pair / triple is assumed to already be\n",
            "      tokenized into a list of strings.\n",
            "    smoothing: (Optional) Float parameter for laplace smoothing.\n",
            "    stopwords: (Optional) List of stopwords to ignore (assign P = 1).\n",
            "\n",
            "  Returns:\n",
            "    prob: Float probability of ngram being entailed by the table.\n",
            "  \"\"\"\n",
            "  # pylint: disable=g-complex-comprehension\n",
            "  if len(table[0]) == 2:\n",
            "    table_values = set([tok for _, value in table for tok in value])\n",
            "  else:\n",
            "    table_values = set([tok for head, _, tail in table for tok in head + tail])\n",
            "  overlap = 0\n",
            "  for token in ngram:\n",
            "    if stopwords is not None and token in stopwords:\n",
            "      overlap += 1\n",
            "      continue\n",
            "    if token in table_values:\n",
            "      overlap += 1\n",
            "  return float(overlap + smoothing) / float(len(ngram) + smoothing)\n",
            "\n",
            "\n",
            "def _mention_probability(table_entry, sentence, smoothing=0.0):\n",
            "  \"\"\"Returns the probability that the table entry is mentioned in the sentence.\n",
            "\n",
            "  A simple implementation which checks the longest common subsequence between\n",
            "  the table entry and the sentence. For tables with (attribute, value) pairs\n",
            "  only the `value` is considered. For tables with (head, relation, tail) triples\n",
            "  a concatenation of the `head` and `tail` is considered.\n",
            "\n",
            "  E.g.:\n",
            "    >>> _mention_probability(([\"name\"], [\"michael\", \"dahlquist\"]),\n",
            "                             [\"michael\", \"dahlquist\", \"was\", \"a\", \"drummer\"])\n",
            "    >>> 1.0\n",
            "\n",
            "  Args:\n",
            "    table_entry: Tuple of either (attribute, value) or (head, relation, tail).\n",
            "      Each member of the tuple is assumed to already be tokenized into a list of\n",
            "      strings.\n",
            "    sentence: List of tokens.\n",
            "    smoothing: Float parameter for laplace smoothing.\n",
            "\n",
            "  Returns:\n",
            "    prob: Float probability of entry being in sentence.\n",
            "  \"\"\"\n",
            "  if len(table_entry) == 2:\n",
            "    value = table_entry[1]\n",
            "  else:\n",
            "    value = table_entry[0] + table_entry[2]\n",
            "  overlap = _len_lcs(value, sentence)\n",
            "  return float(overlap + smoothing) / float(len(value) + smoothing)\n",
            "\n",
            "\n",
            "def _len_lcs(x, y):\n",
            "  \"\"\"Returns the length of the Longest Common Subsequence between two seqs.\n",
            "\n",
            "  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
            "\n",
            "  Args:\n",
            "    x: sequence of words\n",
            "    y: sequence of words\n",
            "\n",
            "  Returns\n",
            "    integer: Length of LCS between x and y\n",
            "  \"\"\"\n",
            "  table = _lcs(x, y)\n",
            "  n, m = len(x), len(y)\n",
            "  return table[n, m]\n",
            "\n",
            "\n",
            "def _lcs(x, y):\n",
            "  \"\"\"Computes the length of the LCS between two seqs.\n",
            "\n",
            "  The implementation below uses a DP programming algorithm and runs\n",
            "  in O(nm) time where n = len(x) and m = len(y).\n",
            "  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
            "\n",
            "  Args:\n",
            "    x: collection of words\n",
            "    y: collection of words\n",
            "\n",
            "  Returns:\n",
            "    Table of dictionary of coord and len lcs\n",
            "  \"\"\"\n",
            "  n, m = len(x), len(y)\n",
            "  table = dict()\n",
            "  for i in range(n + 1):\n",
            "    for j in range(m + 1):\n",
            "      if i == 0 or j == 0:\n",
            "        table[i, j] = 0\n",
            "      elif x[i - 1] == y[j - 1]:\n",
            "        table[i, j] = table[i - 1, j - 1] + 1\n",
            "      else:\n",
            "        table[i, j] = max(table[i - 1, j], table[i, j - 1])\n",
            "  return table\n",
            "\n",
            "\n",
            "def _ngrams(sequence, order):\n",
            "  \"\"\"Yields all ngrams of given order in sequence.\"\"\"\n",
            "  assert order >= 1\n",
            "  for n in range(order, len(sequence) + 1):\n",
            "    yield tuple(sequence[n - order: n])\n",
            "\n",
            "\n",
            "def _ngram_counts(sequence, order):\n",
            "  \"\"\"Returns count of all ngrams of given order in sequence.\"\"\"\n",
            "  if len(sequence) < order:\n",
            "    return collections.Counter()\n",
            "  return collections.Counter(_ngrams(sequence, order))\n",
            "\n",
            "\n",
            "def parent(predictions,\n",
            "           references,\n",
            "           precision_tables,\n",
            "           recall_tables,\n",
            "           lambda_weight=0.5,\n",
            "           smoothing=0.00001,\n",
            "           max_order=4,\n",
            "           entailment_fn=overlap_probability,\n",
            "           mention_fn=_mention_probability):\n",
            "  \"\"\"Metric for comparing predictions to references given tables.\n",
            "\n",
            "  Args:\n",
            "    predictions: An iterator over tokenized predictions. Each prediction is a\n",
            "      list.\n",
            "    references: An iterator over lists of tokenized references. Each prediction\n",
            "      can have multiple references.\n",
            "    precision_tables: An iterator over the tables. Each table is a list of\n",
            "      tuples, where a tuple can either be (attribute, value) pair or (head,\n",
            "      relation, tail) triple. The members of the tuples are assumed to be\n",
            "      themselves tokenized lists of strings. E.g. `[([\"name\"], [\"michael\",\n",
            "      \"dahlquist\"]), ([\"birth\", \"date\"], [\"december\", \"22\", \"1965\"])]` is one\n",
            "      table in the (attribute, value) format with two entries.\n",
            "    recall_tables: An iterator over the tables. Each table is a list of tuples,\n",
            "      where a tuple can either be (attribute, value) pair or (head, relation,\n",
            "      tail) triple. The members of the tuples are assumed to be themselves\n",
            "      tokenized lists of strings. E.g. `[([\"name\"], [\"michael\", \"dahlquist\"]),\n",
            "      ([\"birth\", \"date\"], [\"december\", \"22\", \"1965\"])]` is one table in the\n",
            "      (attribute, value) format with two entries.\n",
            "    lambda_weight: Float weight in [0, 1] to multiply table recall.\n",
            "    smoothing: Float value for replace zero values of precision and recall.\n",
            "    max_order: Maximum order of the ngrams to use.\n",
            "    entailment_fn: A python function for computing the probability that an\n",
            "      ngram is entailed by the table. Its signature should match that of\n",
            "      `overlap_probability` above.\n",
            "    mention_fn: A python function for computing the probability that a\n",
            "      table entry is mentioned in the text. Its signature should\n",
            "        match that of `_mention_probability` above.\n",
            "\n",
            "  Returns:\n",
            "    precision: Average precision of all predictions.\n",
            "    recall: Average recall of all predictions.\n",
            "    f1: Average F-scores of all predictions.\n",
            "    all_f_scores: List of all F-scores for each item.\n",
            "  \"\"\"\n",
            "  precisions, recalls, all_f_scores = [], [], []\n",
            "  reference_recalls, table_recalls = [], []\n",
            "  all_lambdas = []\n",
            "  for prediction, list_of_references, precision_table, recall_table in zip(\n",
            "      predictions, references, precision_tables, recall_tables):\n",
            "    c_prec, c_rec, c_f = [], [], []\n",
            "    ref_rec, table_rec = [], []\n",
            "\n",
            "    # Assert that either single reference or 3 references.\n",
            "    assert len(list_of_references) == 1 or len(list_of_references) == 3\n",
            "\n",
            "    for reference in list_of_references:\n",
            "      # Weighted ngram precisions and recalls for each order.\n",
            "      ngram_prec, ngram_rec = [], []\n",
            "      for order in range(1, max_order + 1):\n",
            "        # Collect n-grams and their entailment probabilities.\n",
            "        pred_ngram_counts = _ngram_counts(prediction, order)\n",
            "        pred_ngram_weights = {\n",
            "            ngram: entailment_fn(ngram, precision_table)\n",
            "            for ngram in pred_ngram_counts\n",
            "        }\n",
            "        ref_ngram_counts = _ngram_counts(reference, order)\n",
            "        ref_ngram_weights = {\n",
            "            ngram: entailment_fn(ngram, precision_table)\n",
            "            for ngram in ref_ngram_counts\n",
            "        }\n",
            "\n",
            "        # Precision.\n",
            "        numerator, denominator = 0., 0.\n",
            "        for ngram, count in pred_ngram_counts.items():\n",
            "          denominator += count\n",
            "          prob_ngram_in_ref = min(\n",
            "              1., float(ref_ngram_counts.get(ngram, 0) / count))\n",
            "          numerator += count * (\n",
            "              prob_ngram_in_ref +\n",
            "              (1. - prob_ngram_in_ref) * pred_ngram_weights[ngram])\n",
            "        if denominator == 0.:\n",
            "          # Set precision to 0.\n",
            "          ngram_prec.append(0.0)\n",
            "        else:\n",
            "          ngram_prec.append(numerator / denominator)\n",
            "\n",
            "        # Recall.\n",
            "        numerator, denominator = 0., 0.\n",
            "        for ngram, count in ref_ngram_counts.items():\n",
            "          prob_ngram_in_pred = min(\n",
            "              1., float(pred_ngram_counts.get(ngram, 0) / count))\n",
            "          denominator += count * ref_ngram_weights[ngram]\n",
            "          numerator += count * ref_ngram_weights[ngram] * prob_ngram_in_pred\n",
            "        if denominator == 0.:\n",
            "          # Set recall to 1.\n",
            "          ngram_rec.append(1.0)\n",
            "        else:\n",
            "          ngram_rec.append(numerator / denominator)\n",
            "\n",
            "      # Compute recall against table fields.\n",
            "      table_mention_probs = [\n",
            "          mention_fn(entry, prediction) for entry in recall_table\n",
            "      ]\n",
            "      table_rec.append(sum(table_mention_probs) / len(recall_table))\n",
            "\n",
            "      # Smoothing.\n",
            "      for order in range(1, max_order):\n",
            "        if ngram_prec[order] == 0.:\n",
            "          ngram_prec[order] = smoothing\n",
            "        if ngram_rec[order] == 0.:\n",
            "          ngram_rec[order] = smoothing\n",
            "\n",
            "      # Compute geometric averages of precision and recall for all orders.\n",
            "      w = 1. / max_order\n",
            "      if any(prec == 0. for prec in ngram_prec):\n",
            "        c_prec.append(0.)\n",
            "      else:\n",
            "        sp = (w * math.log(p_i) for p_i in ngram_prec)\n",
            "        c_prec.append(math.exp(math.fsum(sp)))\n",
            "      if any(rec == 0. for rec in ngram_rec):\n",
            "        ref_rec.append(smoothing)\n",
            "      else:\n",
            "        sr = [w * math.log(r_i) for r_i in ngram_rec]\n",
            "        ref_rec.append(math.exp(math.fsum(sr)))\n",
            "\n",
            "      # Combine reference and table recalls.\n",
            "      if table_rec[-1] == 0.:\n",
            "        table_rec[-1] = smoothing\n",
            "      if ref_rec[-1] == 0. or table_rec[-1] == 0.:\n",
            "        c_rec.append(0.)\n",
            "      else:\n",
            "        if lambda_weight is None:\n",
            "          lw = sum([mention_fn(entry, reference) for entry in recall_table\n",
            "                   ]) / len(recall_table)\n",
            "          lw = 1. - lw\n",
            "        else:\n",
            "          lw = lambda_weight\n",
            "        all_lambdas.append(lw)\n",
            "        c_rec.append(\n",
            "            math.exp((1. - lw) * math.log(ref_rec[-1]) +\n",
            "                     (lw) * math.log(table_rec[-1])))\n",
            "\n",
            "      # F-score.\n",
            "      c_f.append((2. * c_prec[-1] * c_rec[-1]) /\n",
            "                 (c_prec[-1] + c_rec[-1] + 1e-8))\n",
            "\n",
            "    # Get index of best F-score.\n",
            "    max_i = max(enumerate(c_f), key=lambda x: x[1])[0]\n",
            "    precisions.append(c_prec[max_i])\n",
            "    recalls.append(c_rec[max_i])\n",
            "    all_f_scores.append(c_f[max_i])\n",
            "    reference_recalls.append(ref_rec[max_i])\n",
            "    table_recalls.append(table_rec[max_i])\n",
            "\n",
            "  # Multiply by 100 to resemble BLEU output.\n",
            "  avg_precision = 100 * sum(precisions) / len(precisions)\n",
            "  avg_recall = 100 * sum(recalls) / len(recalls)\n",
            "  avg_f_score = 100 * sum(all_f_scores) / len(all_f_scores)\n",
            "  all_f_scores = [100 * f for f in all_f_scores]\n",
            "\n",
            "  return avg_precision, avg_recall, avg_f_score, all_f_scores\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  references = _text_reference_reader(FLAGS.reference_path)\n",
            "  generations = _text_reader(FLAGS.generation_path)\n",
            "  precision_tables = list(_table_reader(FLAGS.precision_table_path))\n",
            "  recall_tables = list(_table_reader(FLAGS.recall_table_path))\n",
            "\n",
            "  assert len(references) == len(generations)\n",
            "  assert len(references) == len(precision_tables)\n",
            "  assert len(references) == len(recall_tables)\n",
            "\n",
            "  precision, recall, f_score, all_f = parent(\n",
            "      generations,\n",
            "      references,\n",
            "      precision_tables,\n",
            "      recall_tables,\n",
            "      lambda_weight=FLAGS.lambda_weight,\n",
            "      smoothing=FLAGS.smoothing,\n",
            "      entailment_fn=overlap_probability)\n",
            "\n",
            "  print(\"Evaluated %d examples.\" % len(all_f))\n",
            "  print(\"Precision = %.2f Recall = %.2f F-score = %.2f\" %\n",
            "        (precision, recall, f_score))\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "  flags.mark_flags_as_required([\n",
            "      \"reference_path\", \"generation_path\", \"precision_table_path\",\n",
            "      \"recall_table_path\"\n",
            "  ])\n",
            "  app.run(main)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. 라이브러리 설치 및 평가 스크립트 준비\n",
        "# ==============================================================================\n",
        "!pip install -q transformers datasets accelerate sentencepiece sacrebleu\n",
        "\n",
        "# PARENT 평가 스크립트 다운로드 (없을 경우를 대비해)\n",
        "!git clone https://github.com/google-research/language.git 2>/dev/null\n",
        "!pip install -r language/language/totto/eval_requirements.txt -q\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import sacrebleu\n",
        "from ast import literal_eval\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    BartTokenizerFast,           # [핵심] 에러 방지용 Fast 토크나이저\n",
        "    BartForConditionalGeneration,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")"
      ],
      "metadata": {
        "id": "Z5QSw9dRmHFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. 데이터 전처리 (스키마 통일 & 99% Cutoff)\n",
        "# ==============================================================================\n",
        "# 파일 경로 (사용자 환경에 맞게 확인 필요)\n",
        "input_train_file = \"/content/totto_data/totto_train_data.jsonl\"\n",
        "filtered_train_file = \"/content/totto_data/totto_train_data_filtered.jsonl\"\n",
        "input_dev_file = \"/content/totto_data/totto_dev_data.jsonl\"\n",
        "clean_dev_file = \"/content/totto_data/totto_dev_data_clean.jsonl\"\n",
        "\n",
        "# 남길 컬럼 정의 (데이터 충돌 방지)\n",
        "target_columns = [\n",
        "    \"table\", \"highlighted_cells\", \"table_page_title\",\n",
        "    \"table_section_title\", \"table_section_text\", \"sentence_annotations\"\n",
        "]\n",
        "\n",
        "print(\"🔄 [1/5] 데이터 정제 및 필터링 시작...\")\n",
        "\n",
        "# 2-1. 학습 데이터 처리\n",
        "if not os.path.exists(filtered_train_file):\n",
        "    df_train = pd.read_json(input_train_file, lines=True)\n",
        "\n",
        "    # 하이라이트 파싱\n",
        "    def parse_highlight(x):\n",
        "        try: return literal_eval(x) if isinstance(x, str) else x\n",
        "        except: return x\n",
        "\n",
        "    if len(df_train) > 0:\n",
        "        df_train[\"highlighted_cells\"] = df_train[\"highlighted_cells\"].apply(parse_highlight)\n",
        "\n",
        "    # 99% Cutoff\n",
        "    counts = df_train[\"highlighted_cells\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
        "    cutoff = counts.quantile(0.99)\n",
        "    df_train = df_train[counts <= cutoff]\n",
        "\n",
        "    # 컬럼 통일 및 저장\n",
        "    df_train = df_train[target_columns]\n",
        "    df_train.to_json(filtered_train_file, orient=\"records\", lines=True)\n",
        "    print(f\"✅ 학습 데이터 저장 완료 (Cutoff: {cutoff})\")\n",
        "\n",
        "# 2-2. 검증 데이터 처리\n",
        "if not os.path.exists(clean_dev_file):\n",
        "    df_dev = pd.read_json(input_dev_file, lines=True)\n",
        "    df_dev = df_dev[target_columns]\n",
        "    df_dev.to_json(clean_dev_file, orient=\"records\", lines=True)\n",
        "    print(\"✅ 검증 데이터 저장 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tam5U--vmY-R",
        "outputId": "62fc573e-36cb-4d2b-ee4c-ba6726a3d791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 [1/5] 데이터 정제 및 필터링 시작...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. Linearizer (테이블 -> 마크다운 변환기)\n",
        "# ==============================================================================\n",
        "class MarkdownLinearizer:\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def _extract_subtable(self, table, highlighted_cells):\n",
        "        if not table or not highlighted_cells: return []\n",
        "\n",
        "        # 관련 행/열 찾기\n",
        "        highlighted_set = set(tuple(cell) for cell in highlighted_cells)\n",
        "        relevant_rows, relevant_cols = set(), set()\n",
        "        for r, c in highlighted_set:\n",
        "            relevant_rows.add(r); relevant_cols.add(c)\n",
        "        if table:\n",
        "            relevant_rows.add(0) # 헤더 포함\n",
        "            for c in range(len(table[0])): relevant_cols.add(c)\n",
        "\n",
        "        subtable = []\n",
        "        for r in sorted(relevant_rows):\n",
        "            if r >= len(table): continue\n",
        "            new_row = []\n",
        "            for c in sorted(relevant_cols):\n",
        "                if c < len(table[r]):\n",
        "                    cell = table[r][c].copy()\n",
        "                    cell['_is_highlighted'] = (r, c) in highlighted_set\n",
        "                    new_row.append(cell)\n",
        "            subtable.append(new_row)\n",
        "        return subtable\n",
        "\n",
        "    def _table_to_markdown(self, subtable):\n",
        "        if not subtable: return \"\"\n",
        "        lines = []\n",
        "        # 헤더 처리\n",
        "        headers = [c.get(\"value\", \"\").strip() or \" \" for c in subtable[0]]\n",
        "        lines.append(\"| \" + \" | \".join(headers) + \" |\")\n",
        "        lines.append(\"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\")\n",
        "\n",
        "        # 데이터 처리\n",
        "        for row in subtable[1:]:\n",
        "            row_vals = []\n",
        "            for i, cell in enumerate(row):\n",
        "                val = cell.get(\"value\", \"\").strip() or \" \"\n",
        "                if cell.get(\"_is_highlighted\", False):\n",
        "                    head = headers[i] if i < len(headers) else \"\"\n",
        "                    val = f\"<selected> {head} : {val} </selected>\" if head else f\"<selected> {val} </selected>\"\n",
        "                row_vals.append(val)\n",
        "            lines.append(\"| \" + \" | \".join(row_vals) + \" |\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def linearize_from_dict(self, data):\n",
        "        parts = [\"Based on the highlighted cells, describe the data.\"] # 기본 Instruction\n",
        "        # 메타데이터 추가\n",
        "        for key, prefix in [('table_page_title', 'Page:'), ('table_section_title', 'Section:')]:\n",
        "            if data.get(key): parts.append(f\"**{prefix}** {data[key].strip()}\")\n",
        "\n",
        "        subtable = self._extract_subtable(data.get(\"table\", []), data.get(\"highlighted_cells\", []))\n",
        "        if subtable:\n",
        "            parts.append(self._table_to_markdown(subtable))\n",
        "        return \"\\n\\n\".join(parts)"
      ],
      "metadata": {
        "id": "x3WfiZoSmbin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. 모델 및 데이터셋 준비\n",
        "# ==============================================================================\n",
        "print(\"🔄 [2/6] 모델 및 데이터셋 준비 중...\")\n",
        "tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-base\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"microsoft/tapex-base\")\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': ['<selected>', '</selected>']})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "linearizer = MarkdownLinearizer()\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": filtered_train_file, \"validation\": clean_dev_file})\n",
        "\n",
        "TRAIN_SIZE = 50000\n",
        "print(f\"⚠️ 학습 데이터를 {TRAIN_SIZE}개로 제한하여 실험합니다.\")\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(TRAIN_SIZE))\n",
        "eval_dataset = dataset[\"validation\"].select(range(1000)) # 검증은 속도를 위해 1000개 유지\n",
        "def preprocess_function(examples):\n",
        "    inputs, targets = [], []\n",
        "    for i in range(len(examples['table'])):\n",
        "        item = {k: examples[k][i] for k in target_columns if k != 'sentence_annotations'}\n",
        "        inputs.append(linearizer.linearize_from_dict(item))\n",
        "        targets.append(examples['sentence_annotations'][i][0]['final_sentence'])\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "print(\"🔄 [3/6] 토크나이징 진행 (병렬 처리)...\")\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True, num_proc=8)\n",
        "tokenized_eval = eval_dataset.map(preprocess_function, batched=True, num_proc=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "19355ce7e4fc4a5380484ff42c4d174b",
            "386612e0533c4beabd247fe8e610c954",
            "9b39dd76c55c4869bf0ccd5df9d9e121",
            "dd2ca803bf7347f28b7376a49ea9ab30",
            "5e7999f2d8ee4b3b8f22561fed82c494",
            "809899dbed704b8583a27baef222ab46",
            "11913da212794e118d99fd16bf8085dc",
            "e55953abdfbe4451b8aa7ba39a5dd707",
            "4386c12c5c5541aa83638085de5c4a90",
            "11f1a2d963b847a4abe1e3fa9af730f5",
            "41bdbe56ef624c8184ca6e61e28339fb",
            "8a0562c9b13544288b4dda339ca0aa30",
            "c8d9fbd2bf9d4bc680dc9a770c259d7b",
            "bcda7041db1d49cb828e0c136f3e0571",
            "3fd448796b6843ebb56ee942c7547d5b",
            "5ab569f2839f4b6a974883ee64cc29da",
            "aaa8ea305b964beabf0dd2f97c429dac",
            "1c7aea324e5341fcba10bdb406ac5068",
            "c643abe3a2fc4be18ec380fe6d7ca563",
            "62ea8b2d4c4042a78312e54c55b8a87a",
            "dac936cf25e84dfe87e070e527e974d5",
            "630ca0c5c1f3470085167e039ff65755"
          ]
        },
        "id": "-oth8hnCmda3",
        "outputId": "d4bcf768-53ee-4c0d-f4fa-6bba1171f1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 [2/6] 모델 및 데이터셋 준비 중...\n",
            "⚠️ 학습 데이터를 50000개로 제한하여 실험합니다.\n",
            "🔄 [3/6] 토크나이징 진행 (병렬 처리)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=8):   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19355ce7e4fc4a5380484ff42c4d174b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a0562c9b13544288b4dda339ca0aa30"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# [진짜_최종_수정] 학습 재개 (OverflowError 원인인 preds의 -100까지 제거)\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import sacrebleu\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "# 1. 메트릭 계산 함수 (여기만 바꾸면 해결됩니다)\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # Pad ID 확보 (없으면 0)\n",
        "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
        "\n",
        "    # [🚨 핵심 수정] labels뿐만 아니라 'preds'도 -100을 없애야 합니다!\n",
        "    # Trainer가 예측값(preds)의 빈 공간도 -100으로 채울 때가 있어서 에러가 났던 것입니다.\n",
        "    if isinstance(preds, np.ndarray):\n",
        "        preds = np.where(preds != -100, preds, pad_token_id)  # <--- 이 줄이 추가됨\n",
        "        preds = preds.tolist()  # 리스트 변환\n",
        "\n",
        "    if isinstance(labels, np.ndarray):\n",
        "        labels = np.where(labels != -100, labels, pad_token_id)\n",
        "        labels = labels.tolist()  # 리스트 변환\n",
        "\n",
        "    # 이제 preds와 labels 모두 깨끗한 정수 리스트이므로 에러가 날 수 없습니다.\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # 빈 문자열 처리\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [label.strip() for label in decoded_labels]\n",
        "\n",
        "    if len(decoded_preds) == 0:\n",
        "        return {\"bleu\": 0.0}\n",
        "\n",
        "    result = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels])\n",
        "    return {\"bleu\": result.score}\n",
        "\n",
        "# 2. 학습 설정 (기존과 동일)\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./tapex_50k_final_fix\",\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=128,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=3e-5,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# 3. 트레이너 다시 만들기\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"🚀 [최종 재시작] 이제 preds의 -100도 제거했으므로 진짜로 멈추지 않습니다.\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "eZsumS-2mfV3",
        "outputId": "cf155178-b728-4ce6-b575-26ed05603c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2656666554.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 [최종 재시작] 이제 preds의 -100도 제거했으므로 진짜로 멈추지 않습니다.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4689/4689 31:12, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.130500</td>\n",
              "      <td>0.206386</td>\n",
              "      <td>36.310435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.148800</td>\n",
              "      <td>0.194237</td>\n",
              "      <td>37.711764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.134900</td>\n",
              "      <td>0.194182</td>\n",
              "      <td>37.820129</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'num_beams': 4, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4689, training_loss=0.13885534900316274, metrics={'train_runtime': 1872.5779, 'train_samples_per_second': 80.103, 'train_steps_per_second': 2.504, 'total_flos': 9.1460468736e+16, 'train_loss': 0.13885534900316274, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 6. PARENT 평가 수행\n",
        "# ==============================================================================\n",
        "print(\"\\n📊 [5/6] PARENT 평가를 위한 파일 생성 중...\")\n",
        "\n",
        "# 1) table_content.txt 생성\n",
        "with open(\"table_content.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i in range(len(eval_dataset)):\n",
        "        t = eval_dataset[i]['table']\n",
        "        h = eval_dataset[i]['highlighted_cells']\n",
        "        cells = []\n",
        "        for r, c in h:\n",
        "            if r < len(t) and c < len(t[r]):\n",
        "                val = t[r][c]['value']\n",
        "                head = t[0][c]['value'] if len(t) > 0 and c < len(t[0]) else \"\"\n",
        "                cells.append(f\"{head}|||{val}\")\n",
        "        f.write(\"\\t\".join(cells) + \"\\n\")\n",
        "\n",
        "# 2) 예측 생성\n",
        "predictions = trainer.predict(tokenized_eval)\n",
        "preds_ids = np.where(predictions.predictions != -100, predictions.predictions, tokenizer.pad_token_id)\n",
        "decoded_preds = tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
        "\n",
        "# 3) generation.txt & reference.txt 생성\n",
        "with open(\"generation.txt\", \"w\", encoding=\"utf-8\") as fg, open(\"reference.txt\", \"w\", encoding=\"utf-8\") as fr:\n",
        "    for pred, ref_ids in zip(decoded_preds, tokenized_eval['labels']):\n",
        "        ref = tokenizer.decode([x for x in ref_ids if x != -100], skip_special_tokens=True)\n",
        "        fg.write(pred.strip() + \"\\n\")\n",
        "        fr.write(ref.strip() + \"\\n\")\n",
        "\n",
        "print(\"📉 [6/6] PARENT 스크립트 실행 결과:\")\n",
        "!python language/language/totto/totto_parent_eval.py \\\n",
        "    --generation_path=generation.txt \\\n",
        "    --reference_path=reference.txt \\\n",
        "    --precision_table_path=table_content.txt \\\n",
        "    --recall_table_path=table_content.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "brE9vIM7mhUO",
        "outputId": "d7ae8e26-53c1-4a02-d614-d8c13aad9d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 [5/6] PARENT 평가를 위한 파일 생성 중...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📉 [6/6] PARENT 스크립트 실행 결과:\n",
            "Evaluated 1000 examples.\n",
            "Precision = 62.21 Recall = 42.47 F-score = 45.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# [결과 확인] 랜덤 샘플 5개 (Input / Prediction / Reference) 출력\n",
        "# ==============================================================================\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def show_inference_results_v2(dataset, model, tokenizer, num_samples=5):\n",
        "    print(f\"\\n🔍 [결과 확인] 총 {len(dataset)}개 중 랜덤 {num_samples}개 샘플 확인\\n\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # 모델 평가 모드\n",
        "    model.eval()\n",
        "    device = model.device\n",
        "\n",
        "    # 랜덤 인덱스 선택\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "\n",
        "    for idx in indices:\n",
        "        item = dataset[idx] # Raw Data (이미 전처리된 상태여야 함)\n",
        "\n",
        "        # 1. 입력 데이터 준비 (Batch 차원 추가)\n",
        "        input_ids = torch.tensor([item['input_ids']]).to(device)\n",
        "\n",
        "        # 2. 모델 생성 (Generate)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids,\n",
        "                max_length=128,\n",
        "                num_beams=5,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        # 3. 디코딩 (Special Token 제거)\n",
        "        # (1) 입력 표 (구조 확인용, Special Token 일부 포함 가능)\n",
        "        input_text = tokenizer.decode(item['input_ids'], skip_special_tokens=True)\n",
        "\n",
        "        # (2) 정답 (Reference) - label에서 -100 제거 후 디코딩\n",
        "        ref_ids = [x for x in item['labels'] if x != -100]\n",
        "        ref_text = tokenizer.decode(ref_ids, skip_special_tokens=True)\n",
        "\n",
        "        # (3) 예측 (Prediction)\n",
        "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # 4. 출력\n",
        "        print(f\"📌 [Index {idx}]\")\n",
        "        print(f\"\\n[입력 (Input Table Content)]:\\n{input_text[:10000]} ... (생략)\") # 너무 길어서 앞부분만 출력\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"\\n[정답 (Reference)]:\\n{ref_text}\")\n",
        "        print(f\"\\n[예측 (Output)]:\\n▶ {pred_text}\")\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# 실행 (검증 데이터셋 'tokenized_eval' 사용)\n",
        "# 만약 'tokenized_eval'이 없다면 위쪽 코드에서 정의된 변수명을 확인해주세요.\n",
        "if 'tokenized_eval' in locals():\n",
        "    show_inference_results_v2(tokenized_eval, model, tokenizer, num_samples=5)\n",
        "else:\n",
        "    print(\"⚠️ 'tokenized_eval' 데이터셋이 정의되지 않았습니다. 학습 코드를 먼저 실행해주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydEpgVsc5T2y",
        "outputId": "ac2d02c1-340a-4d48-9669-95a081c5c6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 [결과 확인] 총 1000개 중 랜덤 5개 샘플 확인\n",
            "\n",
            "================================================================================\n",
            "📌 [Index 250]\n",
            "\n",
            "[입력 (Input Table Content)]:\n",
            "Based on the highlighted cells, describe the data.\n",
            "\n",
            "**Page:** 2012 in AFC\n",
            "\n",
            "**Section:** Events list\n",
            "\n",
            "| # | Event Title | Date | Arena | Location |\n",
            "| --- | --- | --- | --- | --- |\n",
            "| 4 |  Event Title : AFC 3  |  Date : April 14, 2012  |  Arena : Geelong Arena  |  Location : Geelong, Australia  | ... (생략)\n",
            "----------------------------------------\n",
            "\n",
            "[정답 (Reference)]:\n",
            "AFC 3 was an event held on April 14, 2012, at Geelong Arena in Geelong, Australia.\n",
            "\n",
            "[예측 (Output)]:\n",
            "▶ AFC 3 was an event held on April 14, 2012 at Geelong Arena in Geelong, Australia.\n",
            "================================================================================\n",
            "\n",
            "📌 [Index 228]\n",
            "\n",
            "[입력 (Input Table Content)]:\n",
            "Based on the highlighted cells, describe the data.\n",
            "\n",
            "**Page:** Evan Helmuth\n",
            "\n",
            "**Section:** Film\n",
            "\n",
            "| Year | Title | Role | Notes |\n",
            "| --- | --- | --- | --- |\n",
            "|  Year : 2012  |  Title : The Devil Inside  |  Role : Father David Keane  |   | ... (생략)\n",
            "----------------------------------------\n",
            "\n",
            "[정답 (Reference)]:\n",
            "Helmuth was known for his role as Father David Keane in the 2012 film, The Devil Inside.\n",
            "\n",
            "[예측 (Output)]:\n",
            "▶ In 2012, Helmuth played Father David Keane in The Devil Inside.\n",
            "================================================================================\n",
            "\n",
            "📌 [Index 142]\n",
            "\n",
            "[입력 (Input Table Content)]:\n",
            "Based on the highlighted cells, describe the data.\n",
            "\n",
            "**Page:** List of rivers by length\n",
            "\n",
            "**Section:** List of rivers longer than 1000 km\n",
            "\n",
            "|   | River | Length (km) | Length (miles) | Drainage area (km²) | Average discharge (m³/s) | Outflow | Countries in the drainage basin |\n",
            "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
            "|    : 13.  |  River : Mackenzie–Slave–Peace–Finlay  |  Length (km) : 4,241  | 2,637 | 1,790,000 | 10,300 | Beaufort Sea | Canada | ... (생략)\n",
            "----------------------------------------\n",
            "\n",
            "[정답 (Reference)]:\n",
            "Mackenzie River is the thirteenth longest river system in the world and has a total length of 4,241 kilometres including its tributaries.\n",
            "\n",
            "[예측 (Output)]:\n",
            "▶ Mackenzie–Slave–Peace–Finlay is the 13th longest river in Canada.\n",
            "================================================================================\n",
            "\n",
            "📌 [Index 754]\n",
            "\n",
            "[입력 (Input Table Content)]:\n",
            "Based on the highlighted cells, describe the data.\n",
            "\n",
            "**Page:** Linda Sharp\n",
            "\n",
            "**Section:** College\n",
            "\n",
            "| Season | Team | Overall | Conference | Standing | Postseason |\n",
            "| --- | --- | --- | --- | --- | --- |\n",
            "|  Season : 1977–78  | USC | 11–13 | 3–5 | 4th |   |\n",
            "|  Season : 1978–79  | USC | 21–10 | 4–4 | 3rd | WAIAW Regionals |\n",
            "|  Season : 1979–80  | USC | 22–12 | 9–3 | 2nd | AIAW Regionals |\n",
            "|  Season : 1980–81  | USC | 26–8 | 9–3 | 2nd | AIAW Final Four |\n",
            "|  Season : 1981–82  | USC | 23–4 | 9–3 | 2nd | NCAA Regionals |\n",
            "|  Season : 1982–83  | USC | 31–2 | 13–1 | 1st | NCAA Champions |\n",
            "|  Season : 1983–84  | USC | 29–4 | 13–1 | T–1st | NCAA Champions |\n",
            "|  Season : 1984–85  | USC | 21–9 | 10–4 | T–2nd | NCAA Regionals |\n",
            "|  Season : 1985–86  | USC | 31–5 | 8–0 | 1st | NCAA Final Four |\n",
            "|  Season : 1986–87  | USC | 22–8 | 15–3 | 1st | NCAA Regionals |\n",
            "|  Season : 1987–88  | USC | 22–8 | 15–3 | 2nd | NCAA Regionals |\n",
            "|  Season : 1988–89  | USC | 12–16 | 8–10 | T–4th |   |\n",
            "| USC: |  Team : 277–99 (.737)  | 106–40 (.726) |   | ... (생략)\n",
            "----------------------------------------\n",
            "\n",
            "[정답 (Reference)]:\n",
            "In twelve seasons from 1977 to 1989, Sharp accumulated a 271-99 win-loss record.\n",
            "\n",
            "[예측 (Output)]:\n",
            "▶ Linda Sharp compiled a 277–99 record as a head coach.\n",
            "================================================================================\n",
            "\n",
            "📌 [Index 104]\n",
            "\n",
            "[입력 (Input Table Content)]:\n",
            "Based on the highlighted cells, describe the data.\n",
            "\n",
            "**Page:** List of Governors of South Dakota\n",
            "\n",
            "**Section:** Governors of South Dakota\n",
            "\n",
            "| # | Governor (birth and death) | Took office | Left office | Party | City elected from | Lieutenant Governor | Notes |\n",
            "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
            "|  # : 21  | - |   |  Left office : Ralph Herseth (1909–1969)  |  Party : January 6, 1959  |  City elected from : January 3, 1961  | Democratic | Houghton | ... (생략)\n",
            "----------------------------------------\n",
            "\n",
            "[정답 (Reference)]:\n",
            "Ralph Herseth (1909 – 1969) was the 21st Governor of South Dakota from January 6, 1959 to January 3, 1961.\n",
            "\n",
            "[예측 (Output)]:\n",
            "▶ Ralph Herseth served as the 21st Governor of South Dakota from 1959 to 1961.\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}