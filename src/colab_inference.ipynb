{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RnJW5yCU6W3k","outputId":"06dfe147-f4d7-48f3-864d-98e6f23fd21a","executionInfo":{"status":"ok","timestamp":1768962298264,"user_tz":-540,"elapsed":71987,"user":{"displayName":"Ji Hoon Jung","userId":"16862014928268653770"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install torch transformers sacrebleu tqdm sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oFk8z8nQK7QQ","executionInfo":{"status":"ok","timestamp":1768962303189,"user_tz":-540,"elapsed":4926,"user":{"displayName":"Ji Hoon Jung","userId":"16862014928268653770"}},"outputId":"bd955e81-23cc-4550-8426-a18875c5d536"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.6.0-py3-none-any.whl.metadata (39 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.2)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n","Collecting portalocker (from sacrebleu)\n","  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n","Downloading sacrebleu-2.6.0-py3-none-any.whl (100 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.6.0\n"]}]},{"cell_type":"code","source":["import json\n","import torch\n","import collections\n","import sacrebleu\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","# ==========================================\n","# 1. Metric Utility Functions (PARENT & BLEU Helper)\n","# ==========================================\n","def get_ngrams(segment, max_order):\n","    \"\"\"텍스트에서 n-gram 카운트를 추출합니다.\"\"\"\n","    ngram_counts = collections.Counter()\n","    for order in range(1, max_order + 1):\n","        for i in range(0, len(segment) - order + 1):\n","            ngram = tuple(segment[i:i+order])\n","            ngram_counts[ngram] += 1\n","    return ngram_counts\n","\n","def parent_score(predictions, references, tables):\n","    \"\"\"\n","    PARENT 지표 계산 (Precision, Recall, F1)\n","    * 학술적 정의: PARENT는 생성된 텍스트가 테이블의 내용을 얼마나 정확하게(Precision)\n","      그리고 빠짐없이(Recall) 반영했는지를 Reference 문장과 Table 값을 모두 고려하여 평가하는 지표입니다.\n","    \"\"\"\n","    total_precision, total_recall, total_f1 = 0.0, 0.0, 0.0\n","    max_order = 4\n","    smoothing = 1e-13\n","\n","    for pred_text, ref_texts, table_texts in zip(predictions, references, tables):\n","        pred_tokens = pred_text.strip().split()\n","        ref_tokens_list = [ref.strip().split() for ref in ref_texts]\n","\n","        table_tokens = []\n","        for cell_value in table_texts:\n","            table_tokens.extend(str(cell_value).split())\n","\n","        pred_ngrams = get_ngrams(pred_tokens, max_order)\n","        ref_ngrams_list = [get_ngrams(ref, max_order) for ref in ref_tokens_list]\n","        table_ngrams = get_ngrams(table_tokens, max_order)\n","\n","        # Precision\n","        numerator_prec = 0.0\n","        denominator_prec = sum(pred_ngrams.values()) + smoothing\n","        for ngram, count in pred_ngrams.items():\n","            prob_in_table = 1.0 if ngram in table_ngrams else 0.0\n","            prob_in_ref = 0.0\n","            for ref_ngrams in ref_ngrams_list:\n","                prob_in_ref = max(prob_in_ref, min(1.0, ref_ngrams.get(ngram, 0) / count))\n","            w_prob = prob_in_table + prob_in_ref * (1.0 - prob_in_table)\n","            numerator_prec += count * w_prob\n","        precision = numerator_prec / denominator_prec\n","\n","        # Recall\n","        numerator_rec = 0.0\n","        denominator_rec = smoothing\n","        best_recall = 0.0\n","        for ref_ngrams in ref_ngrams_list:\n","            curr_num = 0.0\n","            curr_denom = smoothing\n","            for ngram, count in ref_ngrams.items():\n","                if ngram in table_ngrams:\n","                    curr_denom += count\n","                    if ngram in pred_ngrams:\n","                        curr_num += min(count, pred_ngrams[ngram])\n","            if curr_denom > smoothing:\n","                best_recall = max(best_recall, curr_num / curr_denom)\n","        recall = best_recall\n","\n","        # F1\n","        if precision + recall > 0:\n","            f1 = 2 * precision * recall / (precision + recall)\n","        else:\n","            f1 = 0.0\n","\n","        total_precision += precision\n","        total_recall += recall\n","        total_f1 += f1\n","\n","    n = len(predictions)\n","    return (total_precision / n), (total_recall / n), (total_f1 / n)\n","\n","# ==========================================\n","# 2. Data Loading & Dataset Class\n","# ==========================================\n","class EvaluationDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_input_len=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_input_len = max_input_len\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        input_text = item.get(\"input\", \"\")\n","        model_inputs = self.tokenizer(\n","            input_text,\n","            max_length=self.max_input_len,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","        return {k: v.squeeze(0) for k, v in model_inputs.items()}\n","\n","def load_json_data(path):\n","    data = []\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        if path.endswith(\".jsonl\"):\n","            for line in f:\n","                if line.strip(): data.append(json.loads(line))\n","        else:\n","            data = json.load(f)\n","    return data\n","\n","# ==========================================\n","# 3. Main Inference Logic\n","# ==========================================\n","def run_inference(ckpt_path, data_path, original_dev_path, device=\"cuda\"):\n","    print(f\"Loading model & tokenizer from: {ckpt_path}\")\n","\n","    tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n","    model = AutoModelForSeq2SeqLM.from_pretrained(ckpt_path)\n","    model.to(device)\n","    model.eval()\n","\n","    # 1. 예측 생성 (ID와 함께 저장)\n","    print(\"Loading data & Generating predictions...\")\n","    eval_raw_data = load_json_data(data_path)\n","    eval_dataset = EvaluationDataset(eval_raw_data, tokenizer)\n","    dataloader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n","\n","    pred_dict = {} # {id: prediction_text} 형태로 저장\n","\n","    # 데이터셋의 순서대로 ID를 가져오기 위해 raw_data 참조\n","    all_ids = [item['id'] for item in eval_raw_data]\n","    current_idx = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Generating\"):\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","\n","            generated_ids = model.generate(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                max_length=128,\n","                num_beams=4,\n","                early_stopping=True\n","            )\n","            decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","\n","            # 배치 결과 저장\n","            for text in decoded:\n","                sample_id = all_ids[current_idx]\n","                pred_dict[sample_id] = text.strip()\n","                current_idx += 1\n","\n","    # 2. Reference 로드 및 정렬 (ID 매칭 핵심 로직)\n","    print(\"Loading references & Aligning data...\")\n","\n","    aligned_preds = []\n","    aligned_refs = []\n","    aligned_tables = []\n","\n","    with open(original_dev_path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            ex = json.loads(line)\n","            ex_id = str(ex.get(\"example_id\")) # 원본 데이터의 ID\n","\n","            # [핵심] 예측값 사전에 해당 ID가 있는 경우에만 평가 리스트에 추가\n","            if ex_id in pred_dict:\n","                # 1) Prediction\n","                aligned_preds.append(pred_dict[ex_id])\n","\n","                # 2) Reference\n","                curr_refs = [anno.get(\"final_sentence\", \"\") for anno in ex.get(\"sentence_annotations\", [])]\n","                aligned_refs.append(curr_refs)\n","\n","                # 3) Table (PARENT용)\n","                t_vals = []\n","                for row in ex.get(\"table\", []):\n","                    for cell in row:\n","                        if cell.get(\"value\"): t_vals.append(str(cell.get(\"value\")))\n","                aligned_tables.append(t_vals)\n","\n","    print(f\"Aligned {len(aligned_preds)} samples successfully.\")\n","\n","    # 3. 점수 계산\n","    # BLEU Format Transpose\n","    max_refs = max(len(r) for r in aligned_refs) if aligned_refs else 0\n","    transposed_refs = [[] for _ in range(max_refs)]\n","    for r_list in aligned_refs:\n","        for i in range(max_refs):\n","            transposed_refs[i].append(r_list[i] if i < len(r_list) else \"\")\n","\n","    bleu_score = sacrebleu.corpus_bleu(aligned_preds, transposed_refs).score\n","\n","    # PARENT\n","    p_prec, p_recall, p_f1 = parent_score(aligned_preds, aligned_refs, aligned_tables)\n","\n","    print(\"\\n\" + \"=\"*30)\n","    print(\" Evaluation Results (Fixed)\")\n","    print(\"=\"*30)\n","    print(f\" BLEU Score:       {bleu_score:.2f}\")\n","    print(f\" PARENT Precision: {p_prec * 100:.2f}\")\n","    print(f\" PARENT Recall:    {p_recall * 100:.2f}\")\n","    print(f\" PARENT F1:        {p_f1 * 100:.2f}\")\n","    print(\"=\"*30)\n","\n","# ==========================================\n","# 4. Execution Block\n","# ==========================================\n","if __name__ == \"__main__\":\n","    # 경로 설정 (사용자 환경에 맞게 수정)\n","    CHECKPOINT_PATH = \"/content/drive/MyDrive/models_T5L/checkpoint-19000\"\n","    PREPROCESSED_DATA_PATH = \"/content/drive/MyDrive/totto_dev_LATTICE.json\"\n","    ORIGINAL_DEV_DATA_PATH = \"/content/drive/MyDrive/totto_dev_data.jsonl\"\n","\n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    run_inference(CHECKPOINT_PATH, PREPROCESSED_DATA_PATH, ORIGINAL_DEV_DATA_PATH, DEVICE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLh4ZnwuJGEe","executionInfo":{"status":"ok","timestamp":1768963859666,"user_tz":-540,"elapsed":1556466,"user":{"displayName":"Ji Hoon Jung","userId":"16862014928268653770"}},"outputId":"7ef647a3-757c-42fd-d63d-07be8f6c1390"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model & tokenizer from: /content/drive/MyDrive/models_T5L/checkpoint-19000\n","Loading data & Generating predictions...\n"]},{"output_type":"stream","name":"stderr","text":["Generating: 100%|██████████| 1394/1394 [24:37<00:00,  1.06s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Loading references & Aligning data...\n","Aligned 7700 samples successfully.\n","\n","==============================\n"," Evaluation Results (Fixed)\n","==============================\n"," BLEU Score:       41.07\n"," PARENT Precision: 47.64\n"," PARENT Recall:    76.73\n"," PARENT F1:        54.71\n","==============================\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","import os\n","\n","# 1. 체크포인트 경로 설정 (사용자 경로에 맞게 수정됨)\n","CHECKPOINT_PATH = \"/content/drive/MyDrive/models_T5L/checkpoint-19000\"\n","\n","# 경로가 실제 존재하는지 먼저 확인\n","if not os.path.exists(CHECKPOINT_PATH):\n","    print(f\"❌ 오류: 경로를 찾을 수 없습니다 -> {CHECKPOINT_PATH}\")\n","    print(\"구글 드라이브 마운트가 되어 있는지, 경로 오타가 없는지 확인해주세요.\")\n","else:\n","    print(f\"Loading tokenizer from {CHECKPOINT_PATH}...\")\n","\n","    try:\n","        # 2. 토크나이저 로드\n","        tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH)\n","\n","        # 3. 검증할 토큰 설정\n","        test_token = \"[CLEAN]\"\n","\n","        # 4. 인코딩 수행 (특수 토큰 자동 추가 방지 옵션 필수)\n","        # add_special_tokens=False를 해야 </s> 같은 종료 토큰이 안 붙어서 정확한 확인이 가능합니다.\n","        encoded = tokenizer.encode(test_token, add_special_tokens=False)\n","\n","        print(\"\\n\" + \"=\"*30)\n","        print(\" [Tokenizer Check Result] \")\n","        print(\"=\"*30)\n","        print(f\"Target Token : {test_token}\")\n","        print(f\"Encoded IDs  : {encoded}\")\n","        print(f\"Length       : {len(encoded)}\")\n","\n","        # 5. 결과 판별\n","        if len(encoded) == 1:\n","            print(f\"✅ 성공! '{test_token}'이(가) ID [{encoded[0]}]인 하나의 토큰으로 완벽하게 인식됩니다.\")\n","            print(\"   -> 학습된 토크나이저가 정상적으로 로드되었습니다.\")\n","        else:\n","            print(f\"❌ 실패. '{test_token}'이(가) {len(encoded)}개의 조각으로 쪼개졌습니다.\")\n","            print(\"   -> 체크포인트 폴더에 tokenizer.json, special_tokens_map.json 등이 제대로 저장되지 않았거나\")\n","            print(\"   -> 기본 t5-base 토크나이저가 로드되었을 가능성이 큽니다.\")\n","\n","    except Exception as e:\n","        print(f\"❌ 토크나이저 로드 중 에러 발생: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E464QHLpTnHo","executionInfo":{"status":"ok","timestamp":1768963876991,"user_tz":-540,"elapsed":68,"user":{"displayName":"Ji Hoon Jung","userId":"16862014928268653770"}},"outputId":"8655a3b9-0a56-4124-b2bf-0bfe86dca8da"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading tokenizer from /content/drive/MyDrive/models_T5L/checkpoint-19000...\n","\n","==============================\n"," [Tokenizer Check Result] \n","==============================\n","Target Token : [CLEAN]\n","Encoded IDs  : [32100]\n","Length       : 1\n","✅ 성공! '[CLEAN]'이(가) ID [32100]인 하나의 토큰으로 완벽하게 인식됩니다.\n","   -> 학습된 토크나이저가 정상적으로 로드되었습니다.\n"]}]},{"cell_type":"code","source":["import json\n","import torch\n","import random\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","def inspect_random_samples(ckpt_path, input_path, ref_path, num_samples=5, seed=42, device=\"cuda\"):\n","    \"\"\"\n","    input_path: 모델 입력용 전처리 파일 (예: totto_dev_LATTICE.json)\n","    ref_path:   정답 확인용 원본 파일 (예: totto_dev_data.jsonl)\n","    seed:       매번 같은 랜덤 결과를 보고 싶으면 고정, 아니면 None\n","    \"\"\"\n","    # 랜덤 시드 설정 (재현성을 위해 기본값 42, 매번 다르게 하려면 None으로 설정)\n","    if seed is not None:\n","        random.seed(seed)\n","\n","    print(f\"Loading model from: {ckpt_path}\")\n","    tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n","    model = AutoModelForSeq2SeqLM.from_pretrained(ckpt_path)\n","    model.to(device)\n","    model.eval()\n","\n","    # 1. 입력 데이터 전체 로드 (Preprocessed)\n","    print(f\"Loading inputs from: {input_path}\")\n","    inputs_data = []\n","    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n","        if input_path.endswith(\".jsonl\"):\n","            for line in f:\n","                if line.strip(): inputs_data.append(json.loads(line))\n","        else:\n","            inputs_data = json.load(f)\n","\n","    # [수정된 부분] 전체 데이터에서 무작위로 num_samples 만큼 뽑기\n","    total_len = len(inputs_data)\n","    if total_len > num_samples:\n","        sampled_items = random.sample(inputs_data, num_samples)\n","        print(f\"Randomly selected {num_samples} samples from {total_len} total examples.\")\n","    else:\n","        sampled_items = inputs_data\n","        print(f\"Data size ({total_len}) is smaller than requested samples. Using all data.\")\n","\n","    # 2. 참조 데이터 로드 (Original) - 딕셔너리 변환\n","    print(f\"Loading references from: {ref_path}\")\n","    ref_map = {}\n","    with open(ref_path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            item = json.loads(line)\n","            ex_id = str(item.get(\"example_id\") or item.get(\"id\"))\n","\n","            refs = []\n","            if \"sentence_annotations\" in item:\n","                refs = [ann[\"final_sentence\"] for ann in item[\"sentence_annotations\"]]\n","\n","            if refs:\n","                ref_map[ex_id] = refs\n","\n","    print(f\"\\n{'='*20} Generating Outputs (Random {num_samples}) {'='*20}\\n\")\n","\n","    # 3. 매칭 및 생성 (샘플링된 데이터에 대해서만 수행)\n","    for item in sampled_items:\n","        # 입력 파일에서의 ID와 Input Text\n","        ex_id = str(item.get(\"example_id\") or item.get(\"id\"))\n","        input_text = item.get(\"input\") or item.get(\"source\") or \"\"\n","\n","        # 원본 파일에서 Reference 찾기\n","        references = ref_map.get(ex_id, [])\n","\n","        # 토크나이징 & 생성\n","        model_inputs = tokenizer(\n","            input_text,\n","            return_tensors=\"pt\",\n","            max_length=512,\n","            truncation=True\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **model_inputs,\n","                max_length=128,\n","                num_beams=4,\n","                early_stopping=True\n","            )\n","\n","        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # 출력\n","        print(f\"[Sample ID: {ex_id}]\")\n","        print(f\"▶ Input:      {input_text[:120]} ... (생략)\")\n","        print(f\"▶ Prediction: \\033[94m{prediction}\\033[0m\")\n","        print(f\"▶ References: {references}\")\n","        print(\"-\" * 60)\n","\n","# ==========================================\n","# 실행 설정\n","# ==========================================\n","if __name__ == \"__main__\":\n","    CKPT_PATH = \"/content/drive/MyDrive/models_T5L/checkpoint-19000\"\n","    INPUT_DATA_PATH = \"/content/drive/MyDrive/totto_dev_LATTICE.json\"\n","    REF_DATA_PATH = \"/content/drive/MyDrive/totto_dev_data.jsonl\"\n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    # seed=None으로 설정하면 실행할 때마다 매번 다른 샘플이 나옵니다.\n","    # seed=42 처럼 숫자를 넣으면 항상 고정된 랜덤 샘플이 나옵니다.\n","    inspect_random_samples(\n","        CKPT_PATH,\n","        INPUT_DATA_PATH,\n","        REF_DATA_PATH,\n","        num_samples=5,\n","        seed=None,   # <--- 여기를 None으로 하면 완전 랜덤\n","        device=DEVICE\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VWfalxplcis8","executionInfo":{"status":"ok","timestamp":1768964119347,"user_tz":-540,"elapsed":6588,"user":{"displayName":"Ji Hoon Jung","userId":"16862014928268653770"}},"outputId":"403184b5-b572-43e0-ce04-942b3d9164e1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model from: /content/drive/MyDrive/models_T5L/checkpoint-19000\n","Loading inputs from: /content/drive/MyDrive/totto_dev_LATTICE.json\n","Randomly selected 5 samples from 22293 total examples.\n","Loading references from: /content/drive/MyDrive/totto_dev_data.jsonl\n","\n","==================== Generating Outputs (Random 5) ====================\n","\n","[Sample ID: 4709928858037909194]\n","▶ Input:      [CLEAN] [PAGE] F♯ (musical note) [SEC] Designation by octave [CELL] F♯4 [TYPE] F [R_HEAD] None [C_HEAD] Scientific desig ... (생략)\n","▶ Prediction: \u001b[94mThe F4 has a frequency of 369.994 Hz.\u001b[0m\n","▶ References: ['The frequency of F♯₄ is 369.994 Hz.', 'The frequency of F♯₄ is 369.994 Hz.']\n","------------------------------------------------------------\n","[Sample ID: -530749260696504495]\n","▶ Input:      [CLEAN] [PAGE] North Chevy Chase, Maryland [SEC] Demographics [CELL] 2010 [TYPE] F [R_HEAD] None [C_HEAD] Historical pop ... (생략)\n","▶ Prediction: \u001b[94mThe population of North Chevy Chase was 519 at the 2010 census.\u001b[0m\n","▶ References: ['The population was 519 at the 2010 census in North Chevy Chase, Maryland.', 'The population was 519 at the 2010 census in North Chevy Chase, Maryland.', 'The population was 519 at the 2010 census.']\n","------------------------------------------------------------\n","[Sample ID: -2169512718416516981]\n","▶ Input:      [CLEAN] [PAGE] William H. Boyce [SEC] Almanac [CELL] 1924 [TYPE] F [R_HEAD] None [C_HEAD] Election results [CELL] Willia ... (생략)\n","▶ Prediction: \u001b[94mIn 1924, Boyce defeated Republican nominee Robert G. Houston with 41% of the vote.\u001b[0m\n","▶ References: ['In 1924, Boyce lost to Republican Robert G. Houston.', 'In the 1924 election, Boyce lost to Republican Robert G. Houston.', 'In 1924, Boyce lost to Republican Robert G. Houston.']\n","------------------------------------------------------------\n","[Sample ID: 8482093319204329452]\n","▶ Input:      [CLEAN] [PAGE] Minister for Infrastructure, Transport and Regional Development [SEC] List of ministers for aviation [CEL ... (생략)\n","▶ Prediction: \u001b[94mHarold Thorby was Minister for Transport and Regional Development in Lyons from 24 November 1938.\u001b[0m\n","▶ References: ['In 1938, Lyons appointed Harold Thorby as the first Minister for Aviation.', 'In 1938, Lyons appointed Harold Thorby as the first Minister for Civil Aviation.', 'In 1938, Lyons appointed Harold Thorby as the first Minister for Civil Aviation.']\n","------------------------------------------------------------\n","[Sample ID: -5962230061890538925]\n","▶ Input:      [CLEAN] [PAGE] Luxembourg at the 2006 Winter Olympics [SEC] Figure skating [CELL] Fleur Maxwell [TYPE] F [R_HEAD] None [ ... (생략)\n","▶ Prediction: \u001b[94mFleur Maxwell scored 44.53 points.\u001b[0m\n","▶ References: ['Fleur Maxwell received a score of 44.53 points in the short program for skating.', 'In the short program, Fleur Maxwell received a score of 44.53 points.', 'In the short program, Fleur Maxwell skated and received a score of 44.53 points.']\n","------------------------------------------------------------\n"]}]}]}