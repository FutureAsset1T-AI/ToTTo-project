{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "67090fac",
      "metadata": {
        "id": "67090fac"
      },
      "source": [
        "# TAPEX 기반 ToTTo Table-aware Fact Verification 평가 노트북\n",
        "\n",
        "이 노트북은 다음을 **end-to-end**로 수행합니다.\n",
        "\n",
        "1. **원본 ToTTo 데이터(unlabeled test/dev/train)**에서 `example_id -> table`을 로드\n",
        "2. ToTTo table을 **row_span/column_span을 고려해 2D grid로 복원** 후 `pandas.DataFrame`으로 변환\n",
        "3. **예측 파일(totto_test_predictions.jsonl)**에서 `example_id -> generated_text` 로드\n",
        "4. generated_text를 **문장 단위 claim**으로 분해\n",
        "5. `microsoft/tapex-base-finetuned-tabfact`를 verifier로 사용해 각 claim을 **entailed/refuted** 판정\n",
        "6. 예측별로 **TSS(Table Support Score), HR(Hallucination Rate), TSS_num(숫자 가중)**를 계산\n",
        "7. 결과를 CSV로 저장\n",
        "\n",
        "> 주의: TabFact fine-tuned TAPEX는 *단문 claim*에 최적화되어 있습니다. 본 노트북은 기본적으로 문장 단위로 분해하며, 필요시 숫자/비교/조건 claim 분해를 추가할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "61556815",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61556815",
        "outputId": "e8498fdc-88cc-4e3b-e151-9076346d94e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c20a9508",
      "metadata": {
        "id": "c20a9508"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers accelerate sentencepiece safetensors pandas tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65aafa67",
      "metadata": {
        "id": "65aafa67"
      },
      "source": [
        "## 1. 경로 설정\n",
        "\n",
        "- **원본 테이블 데이터**: test를 평가하려면 보통 `unlabeled_totto_test_data.jsonl` (또는 팀에서 만든 test 원본)\n",
        "- **예측 데이터**: `/content/drive/MyDrive/nlp_project_02/data/totto_test_predictions.jsonl`\n",
        "\n",
        "원본 테이블 파일이 test에 없고 dev/train만 있으면, **predictions의 example_id가 포함된 split과 동일한 원본을 지정**해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "fadc184e",
      "metadata": {
        "id": "fadc184e"
      },
      "outputs": [],
      "source": [
        "# ====== 수정해서 사용 ======\n",
        "DATA_DIR = '/content/drive/MyDrive/nlp_project_02/data'\n",
        "\n",
        "\n",
        "ORIGINAL_TABLE_JSONL = f'{DATA_DIR}/totto_dev_data.jsonl'\n",
        "# ORIGINAL_TABLE_JSONL = f'{DATA_DIR}/totto_preprocessed_dev.json'  # <- JSON이면 아래 로더가 자동 처리\n",
        "\n",
        "# (B) 모델 예측 파일\n",
        "PREDICTIONS_JSONL = f'{DATA_DIR}/totto_test_predictions.jsonl'\n",
        "\n",
        "# 출력 경로\n",
        "OUT_PER_CLAIM_CSV = f'{DATA_DIR}/tapex_eval_per_claim.csv'\n",
        "OUT_PER_EXAMPLE_CSV = f'{DATA_DIR}/tapex_eval_per_example.csv'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "357d32d1",
      "metadata": {
        "id": "357d32d1"
      },
      "source": [
        "## 2. 로더 + 테이블 변환(row_span/col_span 포함)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9126a11c",
      "metadata": {
        "id": "9126a11c"
      },
      "outputs": [],
      "source": [
        "import json, re, os\n",
        "from typing import Dict, Any, List, Iterator, Optional, Tuple, Union\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------- JSON / JSONL 로더 ----------\n",
        "\n",
        "def iter_jsonl(path: str) -> Iterator[Dict[str, Any]]:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if not s:\n",
        "                continue\n",
        "            yield json.loads(s)\n",
        "\n",
        "def iter_json_auto(path: str) -> Iterator[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    .jsonl: line-delimited dict\n",
        "    .json : dict or list[dict]\n",
        "    \"\"\"\n",
        "    if path.endswith('.jsonl'):\n",
        "        yield from iter_jsonl(path)\n",
        "        return\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        obj = json.load(f)\n",
        "\n",
        "    if isinstance(obj, list):\n",
        "        for ex in obj:\n",
        "            if not isinstance(ex, dict):\n",
        "                raise ValueError(f'JSON list item is not dict: {type(ex)}')\n",
        "            yield ex\n",
        "    elif isinstance(obj, dict):\n",
        "        yield obj\n",
        "    else:\n",
        "        raise ValueError(f'Unsupported JSON root type: {type(obj)}')\n",
        "\n",
        "\n",
        "# ---------- ToTTo table -> DataFrame (span-aware) ----------\n",
        "\n",
        "def _make_unique_columns(cols: List[str]) -> List[str]:\n",
        "    seen = {}\n",
        "    out = []\n",
        "    for c in cols:\n",
        "        c = str(c).strip() if c is not None else ''\n",
        "        if c == '':\n",
        "            c = 'col'\n",
        "        if c not in seen:\n",
        "            seen[c] = 0\n",
        "            out.append(c)\n",
        "        else:\n",
        "            seen[c] += 1\n",
        "            out.append(f'{c}_{seen[c]}')\n",
        "    return out\n",
        "\n",
        "def totto_expand_to_grid(table: List[List[Dict[str, Any]]]) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    ToTTo table(List[List[Cell]])을 row_span/column_span을 고려해\n",
        "    직사각형 2D grid(List[List[str]])로 복원.\n",
        "    \"\"\"\n",
        "    grid: List[List[Optional[str]]] = []\n",
        "\n",
        "    def ensure_size(r: int, c: int):\n",
        "        while len(grid) <= r:\n",
        "            grid.append([])\n",
        "        while len(grid[r]) <= c:\n",
        "            grid[r].append(None)\n",
        "\n",
        "    for r, row in enumerate(table):\n",
        "        c = 0\n",
        "        for cell in row:\n",
        "            val = str(cell.get('value', '')).strip()\n",
        "            rs = int(cell.get('row_span', 1) or 1)\n",
        "            cs = int(cell.get('column_span', 1) or 1)\n",
        "\n",
        "            # 이미 span으로 채워진 칸 skip\n",
        "            while True:\n",
        "                ensure_size(r, c)\n",
        "                if grid[r][c] is None:\n",
        "                    break\n",
        "                c += 1\n",
        "\n",
        "            # span 영역 채움\n",
        "            for dr in range(rs):\n",
        "                rr = r + dr\n",
        "                for dc in range(cs):\n",
        "                    cc = c + dc\n",
        "                    ensure_size(rr, cc)\n",
        "                    if grid[rr][cc] is None:\n",
        "                        grid[rr][cc] = val\n",
        "\n",
        "            c += cs\n",
        "\n",
        "    if not grid:\n",
        "        return []\n",
        "\n",
        "    max_len = max(len(r) for r in grid)\n",
        "    out = []\n",
        "    for r in grid:\n",
        "        if len(r) < max_len:\n",
        "            r = r + [None] * (max_len - len(r))\n",
        "        out.append([('' if x is None else str(x)) for x in r])\n",
        "    return out\n",
        "\n",
        "def totto_table_to_df(table: List[List[Dict[str, Any]]]) -> pd.DataFrame:\n",
        "    grid = totto_expand_to_grid(table)\n",
        "    if not grid:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # header row 판단: 원본 첫 row가 전부 is_header=True면 header로 사용\n",
        "    is_header_row = False\n",
        "    if table and table[0]:\n",
        "        flags = [bool(cell.get('is_header', False)) for cell in table[0]]\n",
        "        is_header_row = all(flags)\n",
        "\n",
        "    if is_header_row:\n",
        "        cols = _make_unique_columns(grid[0])\n",
        "        data = grid[1:] if len(grid) > 1 else []\n",
        "        df = pd.DataFrame(data, columns=cols)\n",
        "    else:\n",
        "        cols = _make_unique_columns([f'col_{i}' for i in range(len(grid[0]))])\n",
        "        df = pd.DataFrame(grid, columns=cols)\n",
        "\n",
        "    return df.astype(str)\n",
        "\n",
        "# ---------- example_id -> table df 인덱스 ----------\n",
        "\n",
        "def build_table_index(original_path: str) -> Dict[str, pd.DataFrame]:\n",
        "    idx: Dict[str, pd.DataFrame] = {}\n",
        "    for ex in tqdm(iter_json_auto(original_path), desc='Loading tables'):\n",
        "        eid = str(ex.get('example_id', ex.get('id', ''))).strip()\n",
        "        if not eid:\n",
        "            continue\n",
        "        if 'table' not in ex:\n",
        "            # 전처리 포맷이 다를 수 있음\n",
        "            raise KeyError(f\"No 'table' field in original example (keys={list(ex.keys())[:20]})\")\n",
        "        idx[eid] = totto_table_to_df(ex['table'])\n",
        "    return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff7ae06",
      "metadata": {
        "id": "9ff7ae06"
      },
      "source": [
        "## 3. predictions 로드 (example_id + 텍스트 키 자동 탐지)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ac18ada6",
      "metadata": {
        "id": "ac18ada6"
      },
      "outputs": [],
      "source": [
        "TEXT_KEYS_CANDIDATES = [\n",
        "    'prediction', 'pred', 'output', 'generated', 'generation',\n",
        "    'decoded', 'text', 'hypothesis', 'model_output'\n",
        "]\n",
        "\n",
        "def pick_example_id(ex: Dict[str, Any]) -> str:\n",
        "    for k in ['example_id', 'id', 'guid', 'eid']:\n",
        "        if k in ex and ex[k] is not None:\n",
        "            return str(ex[k])\n",
        "    raise KeyError(f'example_id field not found (keys={list(ex.keys())})')\n",
        "\n",
        "def pick_text_field(ex: Dict[str, Any]) -> str:\n",
        "    # 1) 후보 키 우선\n",
        "    for k in TEXT_KEYS_CANDIDATES:\n",
        "        if k in ex and isinstance(ex[k], str) and ex[k].strip():\n",
        "            return ex[k].strip()\n",
        "    # 2) fallback: 가장 긴 문자열\n",
        "    best = ''\n",
        "    for _, v in ex.items():\n",
        "        if isinstance(v, str) and len(v) > len(best):\n",
        "            best = v\n",
        "    return best.strip()\n",
        "\n",
        "def load_predictions(pred_path: str) -> Dict[str, str]:\n",
        "    preds: Dict[str, str] = {}\n",
        "    for ex in tqdm(iter_json_auto(pred_path), desc=\"Loading predictions\"):  # ✅ auto로 변경\n",
        "        eid = pick_example_id(ex)\n",
        "        txt = pick_text_field(ex)\n",
        "        if eid is None:\n",
        "            continue\n",
        "        preds[str(eid)] = txt\n",
        "    return preds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "353e2629",
      "metadata": {
        "id": "353e2629"
      },
      "source": [
        "## 4. Claim 분해 (generated text → 문장 단위 claims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "cafc6589",
      "metadata": {
        "id": "cafc6589"
      },
      "outputs": [],
      "source": [
        "def get_claims_from_generated_text(text: str):\n",
        "    text = (text or \"\").strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    return [text]  # ✅ 무조건 통째로 1개 claim\n",
        "\n",
        "def is_numeric_claim(claim: str) -> bool:\n",
        "    # 숫자 포함 여부(간단 룰). 필요하면 %, $, 콤마 등 확장 가능\n",
        "    return bool(re.search(r'\\d', claim))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0356afec",
      "metadata": {
        "id": "0356afec"
      },
      "source": [
        "## 5. TAPEX(TabFact) verifier 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "0f630444",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f630444",
        "outputId": "8a75b3d9-d8db-43a3-a4e9-1918ac6b3aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "num_labels: 2\n",
            "id2label: {0: 'Refused', 1: 'Entailed'}\n",
            "model class: <class 'transformers.models.bart.modeling_bart.BartForSequenceClassification'>\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TapexTokenizer, BartForConditionalGeneration\n",
        "\n",
        "MODEL_NAME = \"microsoft/tapex-base-finetuned-tabfact\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device).eval()\n",
        "\n",
        "print(\"device:\", device)\n",
        "print(\"num_labels:\", model.config.num_labels)\n",
        "print(\"id2label:\", model.config.id2label)\n",
        "print(\"model class:\", model.__class__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031edc68",
      "metadata": {
        "id": "031edc68"
      },
      "source": [
        "## 6. Verifier 실행 + 지표 계산\n",
        "\n",
        "### 산출\n",
        "- per-claim 결과: claim별 entail/refute + 확률\n",
        "- per-example 결과: TSS, HR, TSS_num\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "88237bf2",
      "metadata": {
        "id": "88237bf2"
      },
      "outputs": [],
      "source": [
        "from math import isnan\n",
        "import torch\n",
        "from typing import Dict, Optional, Tuple\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 모델/토크나이저 로드 셀에서 이것도 꼭 해둬야 함:\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "@torch.no_grad()\n",
        "def verify_claim(table_df: pd.DataFrame, claim: str) -> Tuple[int, float, str]:\n",
        "    \"\"\"\n",
        "    return (entailed(1)/not(0), entail_prob, pred_label)\n",
        "    id2label: 0=Refused, 1=Entailed\n",
        "    \"\"\"\n",
        "    if table_df is None or table_df.empty or table_df.shape[1] == 0:\n",
        "        return 0, 0.0, \"EmptyTable\"\n",
        "\n",
        "    table_df = table_df.fillna(\"\").astype(str)\n",
        "\n",
        "    enc = tokenizer(\n",
        "        table=table_df,\n",
        "        query=str(claim),\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=1024\n",
        "    )\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    out = model(**enc)                  # logits: [1,2]\n",
        "    probs = torch.softmax(out.logits[0], dim=-1)\n",
        "\n",
        "    entail_id = 1                       # ✅ 고정\n",
        "    entail_prob = float(probs[entail_id].item())\n",
        "\n",
        "    pred_id = int(torch.argmax(probs).item())\n",
        "    entailed = 1 if pred_id == entail_id else 0\n",
        "\n",
        "    pred_label = model.config.id2label.get(pred_id, str(pred_id))\n",
        "    return entailed, entail_prob, pred_label\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(preds: Dict[str, str], tables: Dict[str, pd.DataFrame],\n",
        "             max_examples: Optional[int] = None,\n",
        "             max_claims_per_example: Optional[int] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "\n",
        "    per_claim_rows = []\n",
        "    per_example_rows = []\n",
        "\n",
        "    eids = list(preds.keys())\n",
        "    if max_examples is not None:\n",
        "        eids = eids[:max_examples]\n",
        "\n",
        "    missing = 0\n",
        "\n",
        "    for eid in tqdm(eids, desc='Evaluating'):\n",
        "        if eid not in tables:\n",
        "            missing += 1\n",
        "            continue\n",
        "\n",
        "        table_df = tables[eid]\n",
        "        gen = preds[eid]\n",
        "\n",
        "        # 너희 데이터가 한 문장이라면 get_claims_from_generated_text는 [gen] 반환 권장\n",
        "        claims = get_claims_from_generated_text(gen)\n",
        "\n",
        "        if not claims:\n",
        "            per_example_rows.append({\n",
        "                'example_id': eid,\n",
        "                'num_claims': 0,\n",
        "                'tss': None,\n",
        "                'hr': None,\n",
        "                'tss_num': None,\n",
        "                'pred_label': None,          # ✅ pred_label 변수 없으니 None\n",
        "                'entail_prob_mean': None,    # ✅ (선택) 평균 확률도 None\n",
        "                'generated_text': gen\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        if max_claims_per_example is not None:\n",
        "            claims = claims[:max_claims_per_example]\n",
        "\n",
        "        v = []\n",
        "        w = []\n",
        "        entail_probs = []\n",
        "        pred_labels = []   # per-claim pred_label 모음 (example 요약에 쓰려고)\n",
        "\n",
        "        for ci, c in enumerate(claims):\n",
        "            entailed, entail_prob, pred_label = verify_claim(table_df, c)  # ✅ 3개 받기\n",
        "            numeric = is_numeric_claim(c)\n",
        "            weight = 2.0 if numeric else 1.0\n",
        "\n",
        "            v.append(int(entailed))\n",
        "            w.append(float(weight))\n",
        "            entail_probs.append(float(entail_prob))\n",
        "            pred_labels.append(pred_label)\n",
        "\n",
        "            per_claim_rows.append({\n",
        "                'example_id': eid,\n",
        "                'claim_idx': ci,\n",
        "                'claim': c,\n",
        "                'pred_label': pred_label,        # ✅ 추가\n",
        "                'entailed': int(entailed),\n",
        "                'entail_prob': float(entail_prob),\n",
        "                'is_numeric_claim': int(numeric),\n",
        "                'weight': float(weight)\n",
        "            })\n",
        "\n",
        "        n = len(v)\n",
        "        tss = sum(v) / n\n",
        "        hr = 1.0 - tss\n",
        "        tss_num = (sum(vi * wi for vi, wi in zip(v, w)) / sum(w)) if sum(w) > 0 else None\n",
        "\n",
        "        # per-example pred_label은 claim이 1개일 땐 그거 그대로,\n",
        "        # 여러 개면 \"Entailed/Refused\" 카운트 요약 문자열로 저장\n",
        "        if len(pred_labels) == 1:\n",
        "            pred_label_summary = pred_labels[0]\n",
        "        else:\n",
        "            # 간단 요약: 가장 많이 나온 라벨\n",
        "            from collections import Counter\n",
        "            pred_label_summary = Counter(pred_labels).most_common(1)[0][0]\n",
        "\n",
        "        per_example_rows.append({\n",
        "            'example_id': eid,\n",
        "            'num_claims': n,\n",
        "            'tss': float(tss),\n",
        "            'hr': float(hr),\n",
        "            'tss_num': None if tss_num is None else float(tss_num),\n",
        "            'pred_label': pred_label_summary,                 # ✅ 추가\n",
        "            'entail_prob_mean': float(sum(entail_probs)/len(entail_probs)),  # ✅ 추가(선택)\n",
        "            'generated_text': gen\n",
        "        })\n",
        "\n",
        "    if missing:\n",
        "        print(f'[WARN] {missing} predictions had no matching table in original data. (example_id mismatch or wrong split)')\n",
        "\n",
        "    per_claim_df = pd.DataFrame(per_claim_rows)\n",
        "    per_example_df = pd.DataFrame(per_example_rows)\n",
        "    return per_claim_df, per_example_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e799fd7",
      "metadata": {
        "id": "1e799fd7"
      },
      "source": [
        "## 7. 실행\n",
        "\n",
        "- 먼저 table index를 만들고\n",
        "- predictions를 로드한 다음\n",
        "- 평가를 수행합니다.\n",
        "\n",
        "> 처음에는 `max_examples=20` 정도로 테스트한 뒤, 전체로 늘리는 걸 권장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "12b09c60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12b09c60",
        "outputId": "f858f45d-5101-4703-f5a8-f1f4ba3838eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading tables: 7700it [00:06, 1106.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num tables: 7700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading predictions: 22293it [00:00, 31803.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num predictions: 7700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 20/20 [00:01<00:00, 12.97it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(             example_id  claim_idx  \\\n",
              " 0   7391450717765563190          0   \n",
              " 1   9012083751335522596          0   \n",
              " 2  -8764917516249435941          0   \n",
              " 3  -6915287003153277224          0   \n",
              " 4  -3004901021745997743          0   \n",
              " \n",
              "                                                claim pred_label  entailed  \\\n",
              " 0  Daniel Henry Chamberlain was the 76th Governor...   Entailed         1   \n",
              " 1  In 2016, Alma Jodorowsky played Evelyn in Kids...   Entailed         1   \n",
              " 2                        A. J. Hawk had 119 tackles.   Entailed         1   \n",
              " 3  Peter II the Simple (Pêr II) and Arthur III th...    Refused         0   \n",
              " 4  Ralph J. Parker was the Speaker of the Minneso...   Entailed         1   \n",
              " \n",
              "    entail_prob  is_numeric_claim  weight  \n",
              " 0     0.999975                 1     2.0  \n",
              " 1     0.999974                 1     2.0  \n",
              " 2     0.999974                 1     2.0  \n",
              " 3     0.001682                 1     2.0  \n",
              " 4     0.999462                 0     1.0  ,\n",
              "              example_id  num_claims  tss   hr  tss_num pred_label  \\\n",
              " 0   7391450717765563190           1  1.0  0.0      1.0   Entailed   \n",
              " 1   9012083751335522596           1  1.0  0.0      1.0   Entailed   \n",
              " 2  -8764917516249435941           1  1.0  0.0      1.0   Entailed   \n",
              " 3  -6915287003153277224           1  0.0  1.0      0.0    Refused   \n",
              " 4  -3004901021745997743           1  1.0  0.0      1.0   Entailed   \n",
              " \n",
              "    entail_prob_mean                                     generated_text  \n",
              " 0          0.999975  Daniel Henry Chamberlain was the 76th Governor...  \n",
              " 1          0.999974  In 2016, Alma Jodorowsky played Evelyn in Kids...  \n",
              " 2          0.999974                        A. J. Hawk had 119 tackles.  \n",
              " 3          0.001682  Peter II the Simple (Pêr II) and Arthur III th...  \n",
              " 4          0.999462  Ralph J. Parker was the Speaker of the Minneso...  )"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# 1) 원본 테이블 로드\n",
        "assert os.path.exists(ORIGINAL_TABLE_JSONL), f'Not found: {ORIGINAL_TABLE_JSONL}'\n",
        "assert os.path.exists(PREDICTIONS_JSONL), f'Not found: {PREDICTIONS_JSONL}'\n",
        "\n",
        "tables = build_table_index(ORIGINAL_TABLE_JSONL)\n",
        "print('num tables:', len(tables))\n",
        "\n",
        "# 2) predictions 로드\n",
        "preds = load_predictions(PREDICTIONS_JSONL)\n",
        "print('num predictions:', len(preds))\n",
        "\n",
        "# 3) 평가 (처음엔 소량 테스트 추천)\n",
        "per_claim_df, per_example_df = evaluate(preds, tables, max_examples=20, max_claims_per_example=None)\n",
        "\n",
        "per_claim_df.head(5), per_example_df.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_model_scores(per_claim_df, per_example_df):\n",
        "\n",
        "    macro_tss = per_example_df[\"tss\"].mean()\n",
        "    macro_hr  = 1.0 - macro_tss\n",
        "\n",
        "    micro_tss = per_claim_df[\"entailed\"].mean()\n",
        "    micro_hr  = 1.0 - micro_tss\n",
        "\n",
        "    tss_num = (per_claim_df[\"entailed\"] * per_claim_df[\"weight\"]).sum() / per_claim_df[\"weight\"].sum()\n",
        "    hr_num  = 1.0 - tss_num\n",
        "\n",
        "    entail_prob_mean = per_claim_df[\"entail_prob\"].mean()\n",
        "\n",
        "    label_dist = per_claim_df[\"pred_label\"].value_counts(normalize=True).to_dict()\n",
        "\n",
        "    summary = {\n",
        "        \"N_examples\": int(len(per_example_df)),        # 평가에 사용된 전체 ToTTo example(테이블+생성문) 수\n",
        "        \"N_claims\": int(len(per_claim_df)),            # 전체 검증 claim 수 (현재는 example당 1문장 → N_examples와 거의 동일)\n",
        "\n",
        "        \"Model_TSS_macro\": float(macro_tss),            # (Macro) example 단위 평균 Table Support Score\n",
        "                                                        # = 샘플 하나당 평균적으로 표에 의해 지지되는 비율\n",
        "\n",
        "        \"Model_HR_macro\": float(macro_hr),              # (Macro) example 단위 평균 Hallucination Rate\n",
        "                                                        # = 1 - Model_TSS_macro\n",
        "\n",
        "        \"Model_TSS_micro\": float(micro_tss),            # (Micro) claim 단위 전체 평균 Table Support Score\n",
        "                                                        # = 전체 claim 중 표에 의해 지지되는 비율\n",
        "\n",
        "        \"Model_HR_micro\": float(micro_hr),              # (Micro) claim 단위 전체 Hallucination Rate\n",
        "                                                        # = 1 - Model_TSS_micro\n",
        "\n",
        "        \"Model_TSS_num\": float(tss_num),                # 숫자 포함 claim에 가중치(2)를 둔 weighted Table Support Score\n",
        "                                                        # = 숫자 환각에 더 민감한 사실성 지표\n",
        "\n",
        "        \"Model_HR_num\": float(hr_num),                  # 숫자 가중 hallucination rate\n",
        "                                                        # = 1 - Model_TSS_num\n",
        "\n",
        "        \"EntailProb_mean\": float(entail_prob_mean),     # verifier가 Entailed 클래스로 판단한 평균 확률\n",
        "                                                        # = 모델 사실 판단의 평균 확신도(confidence)\n",
        "\n",
        "        \"LabelDist\": label_dist,                        # verifier 예측 라벨 분포 (Entailed / Refused 비율)\n",
        "                                                        # = 전체 생성문 중 표 기반/비표 기반 비율 요약\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "summary = summarize_model_scores(per_claim_df, per_example_df)\n",
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f8E24SASUQz",
        "outputId": "50417f75-c23e-4618-eca3-e3a1e1d95a55"
      },
      "id": "3f8E24SASUQz",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'N_examples': 20,\n",
              " 'N_claims': 20,\n",
              " 'Model_TSS_macro': 0.75,\n",
              " 'Model_HR_macro': 0.25,\n",
              " 'Model_TSS_micro': 0.75,\n",
              " 'Model_HR_micro': 0.25,\n",
              " 'Model_TSS_num': 0.7631578947368421,\n",
              " 'Model_HR_num': 0.23684210526315785,\n",
              " 'EntailProb_mean': 0.7577358731463392,\n",
              " 'LabelDist': {'Entailed': 0.75, 'Refused': 0.25}}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5093d120",
      "metadata": {
        "id": "5093d120"
      },
      "source": [
        "## 8. 전체 평가 + CSV 저장\n",
        "\n",
        "GPU로 돌리면 훨씬 빠릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3f5c786b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f5c786b",
        "outputId": "5d0b2e67-f577-43be-e7c3-99a89bf77ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 7700/7700 [04:30<00:00, 28.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved: /content/drive/MyDrive/nlp_project_02/data/tapex_eval_per_claim.csv\n",
            "saved: /content/drive/MyDrive/nlp_project_02/data/tapex_eval_per_example.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_examples_scored': 7700,\n",
              " 'avg_tss': 0.8020779220779221,\n",
              " 'avg_hr': 0.19792207792207792,\n",
              " 'avg_tss_num': 0.8020779220779221}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# 전체 평가\n",
        "per_claim_df, per_example_df = evaluate(preds, tables, max_examples=None, max_claims_per_example=None)\n",
        "\n",
        "# 저장\n",
        "per_claim_df.to_csv(OUT_PER_CLAIM_CSV, index=False)\n",
        "per_example_df.to_csv(OUT_PER_EXAMPLE_CSV, index=False)\n",
        "\n",
        "print('saved:', OUT_PER_CLAIM_CSV)\n",
        "print('saved:', OUT_PER_EXAMPLE_CSV)\n",
        "\n",
        "# 요약\n",
        "summary = {\n",
        "    'num_examples_scored': int(per_example_df['tss'].notna().sum()),\n",
        "    'avg_tss': float(per_example_df['tss'].dropna().mean()) if per_example_df['tss'].notna().any() else None,\n",
        "    'avg_hr': float(per_example_df['hr'].dropna().mean()) if per_example_df['hr'].notna().any() else None,\n",
        "    'avg_tss_num': float(per_example_df['tss_num'].dropna().mean()) if per_example_df['tss_num'].notna().any() else None,\n",
        "}\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "547902c8",
      "metadata": {
        "id": "547902c8"
      },
      "source": [
        "## 9. (선택) 평가 신뢰도 체크/디버깅\n",
        "\n",
        "- mismatch가 많으면 **원본 테이블 split을 잘못 잡은 것**일 확률이 큽니다.\n",
        "- predictions의 example_id 일부를 출력해 원본 테이블에 존재하는지 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "13ecf636",
      "metadata": {
        "id": "13ecf636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa10854-cb53-426e-f788-c8e6e6de9bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7391450717765563190 IN_TABLES\n",
            "9012083751335522596 IN_TABLES\n",
            "-8764917516249435941 IN_TABLES\n",
            "-6915287003153277224 IN_TABLES\n",
            "-3004901021745997743 IN_TABLES\n",
            "9095314032876340546 IN_TABLES\n",
            "6803794595179672650 IN_TABLES\n",
            "-2706081572458524575 IN_TABLES\n",
            "3877051823034175640 IN_TABLES\n",
            "6364030237891034315 IN_TABLES\n"
          ]
        }
      ],
      "source": [
        "# mismatch 점검\n",
        "some_ids = list(preds.keys())[:10]\n",
        "for eid in some_ids:\n",
        "    print(eid, 'IN_TABLES' if eid in tables else 'MISSING')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}