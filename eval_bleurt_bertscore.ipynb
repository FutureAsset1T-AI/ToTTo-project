{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "182a645f",
      "metadata": {
        "id": "182a645f"
      },
      "source": [
        "# ToTTo T5 Baseline Evaluation Notebook\n",
        "\n",
        "이 노트북은 `train_t5.ipynb`에서 학습한 **best checkpoint(검증 loss 기준)** 를 로드하여, ToTTo test 데이터에 대해 다음을 수행합니다.\n",
        "\n",
        "1) **Generation (GPU)**\n",
        "2) Generation 결과를 **JSONL 저장** (`example_id`, `prediction`, `references`)\n",
        "3) **BLEURT-base-128 (공식 TensorFlow BLEURT, GPU, multiple reference average aggregation)**\n",
        "4) **BLEU (sacrebleu, multiple reference)**\n",
        "\n",
        "> 주의: BLEURT는 TensorFlow 기반이며, 설치/체크포인트 다운로드에 시간이 걸릴 수 있습니다.\n",
        ">\n",
        "> ToTTo 리더보드 공식 설정: BLEURT-base-128 체크포인트 사용, multiple reference는 average aggregation (Sellam et al. 2020)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9c798cd",
      "metadata": {
        "id": "b9c798cd"
      },
      "source": [
        "## 0. 설치 (Colab/로컬 공용)\n",
        "\n",
        "- Colab에서는 보통 TensorFlow가 기본 설치되어 있습니다.\n",
        "- 로컬에서는 `tensorflow` 설치가 필요할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bTKV92_sZUYd",
      "metadata": {
        "id": "bTKV92_sZUYd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bIcKN4kZydi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bIcKN4kZydi",
        "outputId": "b3d52a02-2c8f-48a8-d985-42f8a9bf4e67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# (선택) Colab에서 드라이브 사용 시\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38ff2a32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38ff2a32",
        "outputId": "1bb89682-e8cc-4e3c-df01-145e144fb01b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# 필수 라이브러리 설치\n",
        "# - BLEURT 공식 구현: google-research/bleurt\n",
        "# - BLEU: sacrebleu\n",
        "# - 모델 로드/생성: transformers\n",
        "\n",
        "!pip -q install --upgrade \"transformers>=4.30\" \"accelerate\" \"sacrebleu>=2.3\" \"tqdm\"\n",
        "!pip -q install --upgrade \"git+https://github.com/google-research/bleurt.git\"\n",
        "!pip -q install --upgrade \"bert-score>=0.3.13\"\n",
        "\n",
        "\n",
        "# (필요 시) tensorflow가 없다면 설치\n",
        "try:\n",
        "    import tensorflow as tf  # noqa: F401\n",
        "except Exception as e:\n",
        "    print('TensorFlow import failed:', e)\n",
        "    print('Installing tensorflow...')\n",
        "    !pip -q install --upgrade \"tensorflow>=2.9\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "296042e6",
      "metadata": {
        "id": "296042e6"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931aa29f",
      "metadata": {
        "id": "931aa29f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import shutil\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "import sacrebleu\n",
        "\n",
        "# BLEURT (공식 TF 구현)\n",
        "from bleurt import score as bleurt_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab9f285",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ab9f285",
        "outputId": "82f95c73-cafc-4b9a-dd42-238ec3ff3ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# TF가 기본적으로 GPU 메모리를 크게 선점하는 것을 방지합니다.\n",
        "# (PyTorch와 같은 프로세스에서 GPU를 같이 쓰는 경우 필수에 가깝습니다.)\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
        "\n",
        "_gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "for _g in _gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(_g, True)\n",
        "    except Exception as _e:\n",
        "        print(\"Could not set TF memory growth:\", _e)\n",
        "\n",
        "print(\"TF visible GPUs:\", _gpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "378a5954",
      "metadata": {
        "id": "378a5954"
      },
      "source": [
        "## 2. 설정 (경로/하이퍼파라미터)\n",
        "\n",
        "아래 경로만 본인 환경에 맞게 수정하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371c55ec",
      "metadata": {
        "id": "371c55ec"
      },
      "outputs": [],
      "source": [
        "# ===== Paths =====\n",
        "# 학습 결과(체크포인트)가 저장된 디렉토리\n",
        "# - train_t5.ipynb의 output_dir과 동일한 디렉토리를 넣는 것을 권장\n",
        "CKPT_DIR = \"/content/drive/MyDrive/nlp_project_02/ckpts-JH-ver1\"\n",
        "\n",
        "# (필수) 테스트 입력 (preprocessed)\n",
        "# - train_t5.ipynb에서 사용한 포맷과 동일하게: input 문자열 포함\n",
        "# - 권장 키: {\"example_id\": ..., \"input\": ...}\n",
        "TEST_PREPROCESSED_PATH = (\n",
        "    \"/content/drive/MyDrive/nlp_project_02/data/totto_preprocessed_test.json\"\n",
        ")\n",
        "\n",
        "# (권장) 원본 ToTTo test JSONL (multiple reference 확보용)\n",
        "# - 파일이 없다면, preprocessed 파일에 references가 포함되어 있어야 합니다.\n",
        "TOTTO_ORIGINAL_TEST_JSONL = (\n",
        "    \"/content/drive/MyDrive/nlp_project_02/data/totto_test_data.jsonl\"\n",
        ")\n",
        "\n",
        "# Generation 결과 저장\n",
        "PRED_JSONL_PATH = (\n",
        "    \"/content/drive/MyDrive/nlp_project_02/preds/totto_test_predictions.jsonl\"\n",
        ")\n",
        "\n",
        "# Metric 결과 저장\n",
        "METRICS_JSON_PATH = (\n",
        "    \"/content/drive/MyDrive/nlp_project_02/preds/totto_test_metrics.json\"\n",
        ")\n",
        "\n",
        "# BLEURT checkpoint cache\n",
        "# ToTTo 리더보드 공식: BLEURT-base-128\n",
        "BLEURT_CACHE_DIR = \"/content/drive/MyDrive/nlp_project_02/bleurt_ckpts\"\n",
        "BLEURT_BASE_128_DIRNAME = \"bleurt-base-128\"\n",
        "\n",
        "# ===== Tokenization / Generation =====\n",
        "MAX_INPUT_LEN = 512\n",
        "MAX_GEN_LEN = 128\n",
        "NUM_BEAMS = 4  # train_t5.ipynb의 generate와 동일\n",
        "\n",
        "# ===== Runtime =====\n",
        "GEN_BATCH_SIZE = 128  # GPU memory에 맞게 조정\n",
        "\n",
        "# ===== Evaluation (GPU) =====\n",
        "# Metric registry에서 모든 GPU metric에 동일 batch_size를 강제합니다.\n",
        "# roberta-large(BERTScore) 기준으로 먼저 안전한 값(예: 8~16)에서 시작하세요.\n",
        "EVAL_BATCH_SIZE = 16\n",
        "\n",
        "PAIR_CHUNK_SIZE = 256  # BLEURT pair 평가 chunk (pair 리스트가 큰 경우 RAM에 맞게 조정)\n",
        "\n",
        "SEED = 42\n",
        "RUN_GENERATION = True\n",
        "RUN_SCORING = True\n",
        "\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c78b67",
      "metadata": {
        "id": "76c78b67"
      },
      "source": [
        "## 3. Baseline과 동일한 유틸리티 (JSON 로더 등)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38731e7c",
      "metadata": {
        "id": "38731e7c"
      },
      "outputs": [],
      "source": [
        "def load_json_or_jsonl(path: str) -> List[Dict]:\n",
        "    if path.endswith(\".jsonl\"):\n",
        "        data = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    data.append(json.loads(line))\n",
        "        return data\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "\n",
        "\n",
        "def detect_best_checkpoint(ckpt_dir: str) -> str:\n",
        "    \"\"\"train_t5.ipynb에서 load_best_model_at_end=True를 사용했으므로\n",
        "    trainer_state.json의 best_model_checkpoint를 우선 사용.\n",
        "    없으면 ckpt_dir 자체를 사용(마지막에 trainer.save_model(output_dir)로 best가 저장되었을 수도 있음).\n",
        "    \"\"\"\n",
        "    state_path = os.path.join(ckpt_dir, \"trainer_state.json\")\n",
        "    if os.path.exists(state_path):\n",
        "        try:\n",
        "            state = json.load(open(state_path, \"r\", encoding=\"utf-8\"))\n",
        "            best = state.get(\"best_model_checkpoint\")\n",
        "            if best and os.path.exists(best):\n",
        "                return best\n",
        "        except Exception:\n",
        "            pass\n",
        "    return ckpt_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b284cd",
      "metadata": {
        "id": "13b284cd"
      },
      "source": [
        "## 4. Test 입력/Reference 로딩\n",
        "\n",
        "- 입력(`input`)은 **preprocessed test**에서 가져옵니다.\n",
        "- reference는 원칙적으로 **원본 ToTTo JSONL의 sentence_annotations**에서 여러 개를 모읍니다.\n",
        "- 만약 원본 JSONL이 없다면, preprocessed 파일에 `references`(list[str])가 포함되어 있어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea03916a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea03916a",
        "outputId": "b67a045e-f4ce-4280-dc9b-3bd6ee9f612f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded inputs: 22293\n",
            "Examples built: 22293\n",
            "Examples with missing references: 0\n",
            "---\n",
            "id: 7391450717765563190\n",
            "input: [PAGE] List of Governors of South Carolina [SEC] Governors under the Constitution of 1868 [TEXT] Parties Democratic Repu\n",
            "n_refs: 1\n",
            "ref0: Daniel Henry Chamberlain was the 76th Governor of South Carolina who took office in 1874.\n",
            "---\n",
            "id: 7391450717765563190\n",
            "input: [PAGE] List of Governors of South Carolina [SEC] Governors under the Constitution of 1868 [TEXT] Parties Democratic Repu\n",
            "n_refs: 1\n",
            "ref0: Daniel Henry Chamberlain was the 76th Governor of South Carolina who took office in 1874.\n",
            "---\n",
            "id: 7391450717765563190\n",
            "input: [PAGE] List of Governors of South Carolina [SEC] Governors under the Constitution of 1868 [TEXT] Parties Democratic Repu\n",
            "n_refs: 1\n",
            "ref0: Daniel Henry Chamberlain was the 76th Governor of South Carolina who took office in 1874.\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class EvalExample:\n",
        "    example_id: str\n",
        "    input_text: str\n",
        "    references: List[str]\n",
        "\n",
        "\n",
        "def load_totto_references_from_original(jsonl_path: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"원본 ToTTo JSONL에서 example_id -> [ref1, ref2, ...] 매핑 생성\n",
        "    - sentence_annotations[*].final_sentence 사용\n",
        "    \"\"\"\n",
        "    ref_map: Dict[str, List[str]] = {}\n",
        "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            ex = json.loads(line)\n",
        "            ex_id = (\n",
        "                ex.get(\"example_id\")\n",
        "                or ex.get(\"id\")\n",
        "                or ex.get(\"table_id\")\n",
        "                or ex.get(\"tableId\")\n",
        "            )\n",
        "            if ex_id is None:\n",
        "                # fallback: 파일 순서 기반 id\n",
        "                ex_id = str(len(ref_map))\n",
        "            ex_id = str(ex_id)\n",
        "\n",
        "            annos = ex.get(\"sentence_annotations\", []) or []\n",
        "            refs = []\n",
        "            for a in annos:\n",
        "                s = (a.get(\"final_sentence\") or \"\").strip()\n",
        "                if s:\n",
        "                    refs.append(s)\n",
        "            if refs:\n",
        "                ref_map[ex_id] = refs\n",
        "\n",
        "    return ref_map\n",
        "\n",
        "\n",
        "def build_eval_examples(\n",
        "    preprocessed_path: str, original_ref_jsonl: Optional[str]\n",
        ") -> List[EvalExample]:\n",
        "    raw = load_json_or_jsonl(preprocessed_path)\n",
        "\n",
        "    # preprocessed input\n",
        "    inputs: List[Tuple[str, str]] = []  # (id, input_text)\n",
        "    row_by_id: Dict[str, Dict] = {}\n",
        "\n",
        "    for i, row in enumerate(raw):\n",
        "        ex_id = row.get(\"example_id\") or row.get(\"id\") or row.get(\"table_id\") or str(i)\n",
        "        ex_id = str(ex_id)\n",
        "        row_by_id[ex_id] = row\n",
        "\n",
        "        inp = (row.get(\"input\") or \"\").strip()\n",
        "        if not inp:\n",
        "            continue\n",
        "        inputs.append((ex_id, inp))\n",
        "\n",
        "    # references\n",
        "    ref_map = None\n",
        "    if original_ref_jsonl and os.path.exists(original_ref_jsonl):\n",
        "        ref_map = load_totto_references_from_original(original_ref_jsonl)\n",
        "\n",
        "    examples: List[EvalExample] = []\n",
        "    missing_ref = 0\n",
        "\n",
        "    for ex_id, inp in inputs:\n",
        "        refs: List[str] = []\n",
        "\n",
        "        # 1) 원본 ToTTo JSONL이 있으면 거기서 references를 가져옴\n",
        "        if ref_map is not None:\n",
        "            refs = ref_map.get(ex_id, [])\n",
        "\n",
        "        # 2) 원본에서 못 찾았거나, 원본 JSONL을 제공하지 않은 경우\n",
        "        #    preprocessed 파일에 references/targets/target이 있으면 활용\n",
        "        if not refs:\n",
        "            row = row_by_id.get(ex_id, {})\n",
        "\n",
        "            if isinstance(row.get(\"references\"), list):\n",
        "                refs = [str(r).strip() for r in row.get(\"references\") if str(r).strip()]\n",
        "            elif isinstance(row.get(\"targets\"), list):\n",
        "                refs = [str(r).strip() for r in row.get(\"targets\") if str(r).strip()]\n",
        "            else:\n",
        "                tgt = (row.get(\"target\") or \"\").strip()\n",
        "                refs = [tgt] if tgt else []\n",
        "\n",
        "        if not refs:\n",
        "            missing_ref += 1\n",
        "\n",
        "        examples.append(EvalExample(example_id=ex_id, input_text=inp, references=refs))\n",
        "\n",
        "    print(f\"Loaded inputs: {len(inputs)}\")\n",
        "    if ref_map is not None:\n",
        "        print(f\"Loaded reference map: {len(ref_map)}\")\n",
        "    print(f\"Examples built: {len(examples)}\")\n",
        "    print(f\"Examples with missing references: {missing_ref}\")\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "examples = build_eval_examples(TEST_PREPROCESSED_PATH, TOTTO_ORIGINAL_TEST_JSONL)\n",
        "\n",
        "# sanity check\n",
        "for ex in examples[:3]:\n",
        "    print(\"---\")\n",
        "    print(\"id:\", ex.example_id)\n",
        "    print(\"input:\", ex.input_text[:120].replace(\"\\\\n\", \" \"))\n",
        "    print(\"n_refs:\", len(ex.references))\n",
        "    if ex.references:\n",
        "        print(\"ref0:\", ex.references[0][:120])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bca91949",
      "metadata": {
        "id": "bca91949"
      },
      "source": [
        "## 5. Tokenizer/Model 로드 (best checkpoint 우선)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6979b593",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6979b593",
        "outputId": "55f49f5a-7bf0-4957-e456-affd807a8879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using checkpoint: /content/drive/MyDrive/nlp_project_02/ckpts-JH-ver1\n",
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "best_ckpt = detect_best_checkpoint(CKPT_DIR)\n",
        "print(\"Using checkpoint:\", best_ckpt)\n",
        "\n",
        "# train_t5.ipynb에서는 t5-base로 시작했으나, 저장된 ckpt에서 tokenizer/model을 로드하는 것이 가장 안전\n",
        "tokenizer = AutoTokenizer.from_pretrained(best_ckpt)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(best_ckpt)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd9a5bb",
      "metadata": {
        "id": "0cd9a5bb"
      },
      "source": [
        "## 6. Generation (GPU) + JSONL 저장\n",
        "\n",
        "- 출력 스키마(요청하신 최소 스키마):\n",
        "  - `example_id`\n",
        "  - `prediction`\n",
        "  - `references` (list[str])\n",
        "\n",
        "- multiple reference는 `TOTTO_ORIGINAL_TEST_JSONL`에서 가져온 것을 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sKTSRJGSfMpO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKTSRJGSfMpO",
        "outputId": "b9d89d43-16e8-4327-ef56-b8e6dae8f3b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 20 samples...\n",
            "\n",
            "================================================================================\n",
            "[Sample 1/20] ID: 7391450717765563190\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] List of Governors of South Carolina [SEC] Governors under the Constitution of 1868 [TEXT] Parties Democratic Republican [CELL] [H] 76 [/H] [TYPE] T [R_HEAD] None [C_HEAD] None [CELL] [H] Daniel Henry Chamberlain [/H] [TYPE] F [R_HEAD] 76 [C_HEAD] Took Office [CELL] [H] December 1, 1874 [/H] [TYPE] F [R_HEAD] 76 [C_HEAD] Left Office\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "Daniel Henry Chamberlain served as the 76th Governor of South Carolina from 1874 to 1874.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Daniel Henry Chamberlain was the 76th Governor of South Carolina who took office in 1874.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 2/20] ID: 7391450717765563190\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] List of Governors of South Carolina [SEC] Governors under the Constitution of 1868 [TEXT] Parties Democratic Republican [CELL] [H] 76 [/H] [TYPE] T [R_HEAD] None [C_HEAD] None [CELL] [H] Daniel Henry Chamberlain [/H] [TYPE] F [R_HEAD] 76 [C_HEAD] Took Office [CELL] [H] December 1, 1874 [/H] [TYPE] F [R_HEAD] 76 [C_HEAD] Left Office\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "Daniel Henry Chamberlain served as the 76th Governor of South Carolina from 1874 to 1874.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Daniel Henry Chamberlain was the 76th Governor of South Carolina who took office in 1874.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 3/20] ID: 7391450717765563190\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] List of Governors of South Carolina [SEC] Governors under the Constitution of 1868 [TEXT] Parties Democratic Republican [CELL] [H] 76 [/H] [TYPE] T [R_HEAD] None [C_HEAD] None [CELL] [H] Daniel Henry Chamberlain [/H] [TYPE] F [R_HEAD] 76 [C_HEAD] Took Office [CELL] [H] December 1, 1874 [/H] [TYPE] F [R_HEAD] 76 [C_HEAD] Left Office\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "Daniel Henry Chamberlain served as the 76th Governor of South Carolina from 1874 to 1874.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Daniel Henry Chamberlain was the 76th Governor of South Carolina who took office in 1874.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 4/20] ID: 9012083751335522596\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] Alma Jodorowsky [SEC] Filmography [TEXT] None [CELL] [H] 2016 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Kids in Love [/H] [TYPE] F [R_HEAD] None [C_HEAD] Title [CELL] [H] Evelyn [/H] [TYPE] F [R_HEAD] None [C_HEAD] Role\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "In 2016, Alma Jodorowsky appeared as Evelyn in the film Kids in Love.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Alma Jodorowsky played the role of Evelyn in the 2016 film Kids in Love.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 5/20] ID: 9012083751335522596\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] Alma Jodorowsky [SEC] Filmography [TEXT] None [CELL] [H] 2016 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Kids in Love [/H] [TYPE] F [R_HEAD] None [C_HEAD] Title [CELL] [H] Evelyn [/H] [TYPE] F [R_HEAD] None [C_HEAD] Role\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "In 2016, Alma Jodorowsky appeared as Evelyn in the film Kids in Love.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Alma Jodorowsky played the role of Evelyn in the 2016 film Kids in Love.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 6/20] ID: 9012083751335522596\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] Alma Jodorowsky [SEC] Filmography [TEXT] None [CELL] [H] 2016 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Kids in Love [/H] [TYPE] F [R_HEAD] None [C_HEAD] Title [CELL] [H] Evelyn [/H] [TYPE] F [R_HEAD] None [C_HEAD] Role\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "In 2016, Alma Jodorowsky appeared as Evelyn in the film Kids in Love.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Alma Jodorowsky played the role of Evelyn in the 2016 film Kids in Love.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 7/20] ID: -8764917516249435941\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] A. J. Hawk [SEC] Career statistics [TEXT] Key GP: games played COMB: combined tackles TOTAL: total tackles AST: assisted tackles SACK: sacks FF: forced fumbles FR: fumble recoveries FR YDS: fumble return yards INT: interceptions IR YDS: interception return yards AVG IR: average interception return LNG: longest interception return TD: interceptions returned for touchdown PD: passes defensed [CELL] [H] 119 [/H] [TYPE] F [R_HEAD] None [C_HEAD] TOTAL\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "A. J. Hawk had 119 total tackles.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Hawk made 119 total tackles, In his rookie season.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 8/20] ID: -8764917516249435941\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] A. J. Hawk [SEC] Career statistics [TEXT] Key GP: games played COMB: combined tackles TOTAL: total tackles AST: assisted tackles SACK: sacks FF: forced fumbles FR: fumble recoveries FR YDS: fumble return yards INT: interceptions IR YDS: interception return yards AVG IR: average interception return LNG: longest interception return TD: interceptions returned for touchdown PD: passes defensed [CELL] [H] 119 [/H] [TYPE] F [R_HEAD] None [C_HEAD] TOTAL\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "A. J. Hawk had 119 total tackles.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Hawk made 119 total tackles, In his rookie season.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 9/20] ID: -8764917516249435941\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] A. J. Hawk [SEC] Career statistics [TEXT] Key GP: games played COMB: combined tackles TOTAL: total tackles AST: assisted tackles SACK: sacks FF: forced fumbles FR: fumble recoveries FR YDS: fumble return yards INT: interceptions IR YDS: interception return yards AVG IR: average interception return LNG: longest interception return TD: interceptions returned for touchdown PD: passes defensed [CELL] [H] 119 [/H] [TYPE] F [R_HEAD] None [C_HEAD] TOTAL\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "A. J. Hawk had 119 total tackles.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Hawk made 119 total tackles, In his rookie season.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 10/20] ID: -6915287003153277224\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] List of rulers of Brittany [SEC] House of Montfort [TEXT] None [CELL] [H] Peter II the Simple (Pêr II) 1450–1457 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Name [CELL] [H] Arthur III the Justicier (Arzhur III) 1457–1458 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Name [CELL] [H] 26 December 1458 Nantes aged 65 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Death\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "Peter II the Simple (Pêr II) 1450–1457 and Arthur III the Justicier (Arzhur III) 1457–1458 were the rulers of Brittany.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Arthur III was a duke of Brittany.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 11/20] ID: -6915287003153277224\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] List of rulers of Brittany [SEC] House of Montfort [TEXT] None [CELL] [H] Peter II the Simple (Pêr II) 1450–1457 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Name [CELL] [H] Arthur III the Justicier (Arzhur III) 1457–1458 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Name [CELL] [H] 26 December 1458 Nantes aged 65 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Death\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "Peter II the Simple (Pêr II) 1450–1457 and Arthur III the Justicier (Arzhur III) 1457–1458 were the rulers of Brittany.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Arthur III was a duke of Brittany.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 12/20] ID: -3004901021745997743\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] List of Speakers of the Minnesota House of Representatives [SEC] State [TEXT] None [CELL] [H] Ralph J. Parker [/H] [TYPE] F [R_HEAD] None [C_HEAD] Speaker\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "Ralph J. Parker was the Speaker of the Minnesota House of Representatives.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Ralph J. Parke, was a Minnesota politician, and a Speaker of the Minnesota House of Representatives.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 13/20] ID: -3004901021745997743\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] List of Speakers of the Minnesota House of Representatives [SEC] State [TEXT] None [CELL] [H] Ralph J. Parker [/H] [TYPE] F [R_HEAD] None [C_HEAD] Speaker\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "Ralph J. Parker was the Speaker of the Minnesota House of Representatives.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Ralph J. Parke, was a Minnesota politician, and a Speaker of the Minnesota House of Representatives.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 14/20] ID: -3004901021745997743\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] List of Speakers of the Minnesota House of Representatives [SEC] State [TEXT] None [CELL] [H] Ralph J. Parker [/H] [TYPE] F [R_HEAD] None [C_HEAD] Speaker\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "Ralph J. Parker was the Speaker of the Minnesota House of Representatives.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Ralph J. Parke, was a Minnesota politician, and a Speaker of the Minnesota House of Representatives.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 15/20] ID: 9095314032876340546\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] Asian Beach Games [SEC] List of Asian Beach Games [TEXT] None [CELL] [H] 2016 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Da Nang [/H] [TYPE] F [R_HEAD] None [C_HEAD] Host City [CELL] [H] Vietnam [/H] [TYPE] F [R_HEAD] None [C_HEAD] Host Nation [CELL] [H] 24 September [/H] [TYPE] F [R_HEAD] None [C_HEAD] Start Date [CELL] [H] 3 October [/H] [TYPE] F [R_HEAD] None [C_HEAD] End Date [CELL] [H] 2020 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Sanya [/H] [TYPE] F [R_HEAD] No...\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "The Asian Beach Games were held in Da Nang, Vietnam in 2016 and Sanya, China in 2020.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Asian Beach Games were held in Danang, Vietnam from 24 September to 3 October 2016, while the next games held in 2020 in Sanya, China\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 16/20] ID: 9095314032876340546\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] Asian Beach Games [SEC] List of Asian Beach Games [TEXT] None [CELL] [H] 2016 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Da Nang [/H] [TYPE] F [R_HEAD] None [C_HEAD] Host City [CELL] [H] Vietnam [/H] [TYPE] F [R_HEAD] None [C_HEAD] Host Nation [CELL] [H] 24 September [/H] [TYPE] F [R_HEAD] None [C_HEAD] Start Date [CELL] [H] 3 October [/H] [TYPE] F [R_HEAD] None [C_HEAD] End Date [CELL] [H] 2020 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Sanya [/H] [TYPE] F [R_HEAD] No...\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "The Asian Beach Games were held in Da Nang, Vietnam in 2016 and Sanya, China in 2020.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Asian Beach Games were held in Danang, Vietnam from 24 September to 3 October 2016, while the next games held in 2020 in Sanya, China\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 17/20] ID: 9095314032876340546\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] Asian Beach Games [SEC] List of Asian Beach Games [TEXT] None [CELL] [H] 2016 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Da Nang [/H] [TYPE] F [R_HEAD] None [C_HEAD] Host City [CELL] [H] Vietnam [/H] [TYPE] F [R_HEAD] None [C_HEAD] Host Nation [CELL] [H] 24 September [/H] [TYPE] F [R_HEAD] None [C_HEAD] Start Date [CELL] [H] 3 October [/H] [TYPE] F [R_HEAD] None [C_HEAD] End Date [CELL] [H] 2020 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Sanya [/H] [TYPE] F [R_HEAD] No...\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "The Asian Beach Games were held in Da Nang, Vietnam in 2016 and Sanya, China in 2020.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "Asian Beach Games were held in Danang, Vietnam from 24 September to 3 October 2016, while the next games held in 2020 in Sanya, China\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 18/20] ID: 6803794595179672650\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] Julian Sands [SEC] Television [TEXT] None [CELL] [H] 2012 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Person of Interest [/H] [TYPE] F [R_HEAD] None [C_HEAD] Title [CELL] [H] Alistair Wesley [/H] [TYPE] F [R_HEAD] None [C_HEAD] Role [CELL] [H] Episode: Critical [/H] [TYPE] F [R_HEAD] None [C_HEAD] Notes\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "In 2012, Julian Sands played as Alistair Wesley in the episode \"Person of Interest\" in Person of Interest.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "In 2012, Julian Sands played as Alistair Wesley in the Person of Interest.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 19/20] ID: 6803794595179672650\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] Julian Sands [SEC] Television [TEXT] None [CELL] [H] 2012 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Person of Interest [/H] [TYPE] F [R_HEAD] None [C_HEAD] Title [CELL] [H] Alistair Wesley [/H] [TYPE] F [R_HEAD] None [C_HEAD] Role [CELL] [H] Episode: Critical [/H] [TYPE] F [R_HEAD] None [C_HEAD] Notes\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "In 2012, Julian Sands played as Alistair Wesley in the episode \"Person of Interest\" in Person of Interest.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "In 2012, Julian Sands played as Alistair Wesley in the Person of Interest.\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Sample 20/20] ID: 6803794595179672650\n",
            "--------------------------------------------------------------------------------\n",
            "INPUT:\n",
            "[PAGE] Julian Sands [SEC] Television [TEXT] None [CELL] [H] 2012 [/H] [TYPE] F [R_HEAD] None [C_HEAD] Year [CELL] [H] Person of Interest [/H] [TYPE] F [R_HEAD] None [C_HEAD] Title [CELL] [H] Alistair Wesley [/H] [TYPE] F [R_HEAD] None [C_HEAD] Role [CELL] [H] Episode: Critical [/H] [TYPE] F [R_HEAD] None [C_HEAD] Notes\n",
            "--------------------------------------------------------------------------------\n",
            "PREDICTION:\n",
            "In 2012, Julian Sands played as Alistair Wesley in the episode \"Person of Interest\" in Person of Interest.\n",
            "--------------------------------------------------------------------------------\n",
            "REFERENCE:\n",
            "In 2012, Julian Sands played as Alistair Wesley in the Person of Interest.\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ===== 샘플 데이터 생성 및 출력 =====\n",
        "# 전체 generation 없이 일부 샘플만 빠르게 생성하여 확인합니다.\n",
        "\n",
        "NUM_SAMPLES = 20  # 생성할 샘플 수\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_samples(\n",
        "    examples: List[EvalExample],\n",
        "    tokenizer,\n",
        "    model,\n",
        "    num_samples: int = 10,\n",
        "    max_input_len: int = 512,\n",
        "    max_gen_len: int = 128,\n",
        "    num_beams: int = 4,\n",
        "):\n",
        "    \"\"\"일부 샘플만 생성하여 출력합니다.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 샘플 수 제한\n",
        "    sample_examples = examples[:num_samples]\n",
        "\n",
        "    print(f\"Generating {len(sample_examples)} samples...\\n\")\n",
        "\n",
        "    for i, ex in enumerate(sample_examples):\n",
        "        # Tokenize\n",
        "        tok = tokenizer(\n",
        "            ex.input_text,\n",
        "            max_length=max_input_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = tok[\"input_ids\"].to(model.device)\n",
        "        attention_mask = tok[\"attention_mask\"].to(model.device)\n",
        "\n",
        "        # Generate\n",
        "        gen_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_gen_len,\n",
        "            num_beams=num_beams,\n",
        "        )\n",
        "\n",
        "        pred = tokenizer.decode(gen_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        # 출력\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"[Sample {i + 1}/{num_samples}] ID: {ex.example_id}\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"INPUT:\\n{ex.input_text[:500]}{'...' if len(ex.input_text) > 500 else ''}\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"PREDICTION:\\n{pred}\")\n",
        "        print(\"-\" * 80)\n",
        "        if ex.references:\n",
        "            print(f\"REFERENCE:\\n{ex.references[0]}\")\n",
        "        else:\n",
        "            print(\"REFERENCE: (없음)\")\n",
        "        print(\"=\" * 80)\n",
        "        print()\n",
        "\n",
        "\n",
        "# 실행\n",
        "generate_samples(\n",
        "    examples=examples,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    num_samples=NUM_SAMPLES,\n",
        "    max_input_len=MAX_INPUT_LEN,\n",
        "    max_gen_len=MAX_GEN_LEN,\n",
        "    num_beams=NUM_BEAMS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5109f9b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5109f9b4",
        "outputId": "97c451a4-90f0-4c95-ab4a-0f8ded41f6db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating: 100%|██████████| 175/175 [19:29<00:00,  6.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 22293 predictions to: /content/drive/MyDrive/nlp_project_02/preds/totto_test_predictions.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "class InputOnlyDataset(Dataset):\n",
        "    def __init__(self, examples: List[EvalExample], tokenizer, max_input_len: int):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_len = max_input_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx]\n",
        "        tok = self.tokenizer(\n",
        "            ex.input_text,\n",
        "            max_length=self.max_input_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in tok.items()}\n",
        "        item[\"example_id\"] = ex.example_id\n",
        "        return item\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_and_save_jsonl(\n",
        "    examples: List[EvalExample],\n",
        "    tokenizer,\n",
        "    model,\n",
        "    pred_jsonl_path: str,\n",
        "    batch_size: int,\n",
        "    max_input_len: int,\n",
        "    max_gen_len: int,\n",
        "    num_beams: int,\n",
        "):\n",
        "    ensure_dir(pred_jsonl_path)\n",
        "\n",
        "    # 이미 생성된 파일이 있다면 건너뛰기\n",
        "    if os.path.exists(pred_jsonl_path):\n",
        "        print(f\"Predictions already exist at {pred_jsonl_path}. Skipping generation.\")\n",
        "        return\n",
        "\n",
        "    dataset = InputOnlyDataset(examples, tokenizer, max_input_len=max_input_len)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    n_written = 0\n",
        "    ref_by_id = {ex.example_id: ex.references for ex in examples}\n",
        "\n",
        "    with open(pred_jsonl_path, \"w\", encoding=\"utf-8\") as wf:\n",
        "        for batch in tqdm(loader, desc=\"Generating\"):\n",
        "            example_ids = batch.pop(\"example_id\")\n",
        "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
        "\n",
        "            gen_ids = model.generate(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch.get(\"attention_mask\", None),\n",
        "                max_length=max_gen_len,\n",
        "                num_beams=num_beams,\n",
        "            )\n",
        "\n",
        "            preds = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
        "            preds = [p.strip() for p in preds]\n",
        "\n",
        "            for ex_id, pred in zip(example_ids, preds):\n",
        "                # references는 원본 examples에서 찾아 넣기 (O(1) dict lookup)\n",
        "                refs = ref_by_id.get(str(ex_id), [])\n",
        "\n",
        "                obj = {\n",
        "                    \"example_id\": str(ex_id),\n",
        "                    \"prediction\": pred,\n",
        "                    \"references\": refs,\n",
        "                }\n",
        "                wf.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
        "                n_written += 1\n",
        "\n",
        "    print(f\"Saved {n_written} predictions to: {pred_jsonl_path}\")\n",
        "\n",
        "\n",
        "if RUN_GENERATION:\n",
        "    generate_and_save_jsonl(\n",
        "        examples=examples,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        pred_jsonl_path=PRED_JSONL_PATH,\n",
        "        batch_size=GEN_BATCH_SIZE,\n",
        "        max_input_len=MAX_INPUT_LEN,\n",
        "        max_gen_len=MAX_GEN_LEN,\n",
        "        num_beams=NUM_BEAMS,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nuPEVoG-dDdI",
      "metadata": {
        "id": "nuPEVoG-dDdI"
      },
      "outputs": [],
      "source": [
        "# # ===== [H][/H] 태그 제거 (Post-processing) =====\n",
        "# # ToTTo 모델이 생성한 prediction에서 highlight 태그를 제거합니다.\n",
        "\n",
        "# import re\n",
        "\n",
        "# def strip_highlight_tags(text: str) -> str:\n",
        "#     \"\"\"[H] 와 [/H] 태그를 제거하고, 불필요한 공백을 정리합니다.\"\"\"\n",
        "#     # [H] 와 [/H] 태그 제거\n",
        "#     text = re.sub(r'\\[H\\]\\s*', '', text)\n",
        "#     text = re.sub(r'\\s*\\[/H\\]', '', text)\n",
        "#     # 연속된 공백을 단일 공백으로\n",
        "#     text = re.sub(r'\\s+', ' ', text)\n",
        "#     return text.strip()\n",
        "\n",
        "\n",
        "# def clean_predictions_jsonl(input_path: str, output_path: str = None):\n",
        "#     \"\"\"JSONL 파일의 prediction 필드에서 [H][/H] 태그를 제거합니다.\"\"\"\n",
        "#     if output_path is None:\n",
        "#         output_path = input_path  # 덮어쓰기\n",
        "\n",
        "#     cleaned_rows = []\n",
        "#     with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#         for line in f:\n",
        "#             line = line.strip()\n",
        "#             if line:\n",
        "#                 row = json.loads(line)\n",
        "#                 if \"prediction\" in row:\n",
        "#                     row[\"prediction\"] = strip_highlight_tags(row[\"prediction\"])\n",
        "#                 cleaned_rows.append(row)\n",
        "\n",
        "#     with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "#         for row in cleaned_rows:\n",
        "#             f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "#     print(f\"Cleaned {len(cleaned_rows)} predictions. Saved to: {output_path}\")\n",
        "\n",
        "\n",
        "# # 실행 (기존 파일 덮어쓰기)\n",
        "# clean_predictions_jsonl(PRED_JSONL_PATH)\n",
        "\n",
        "# # 결과 확인 (처음 5개)\n",
        "# with open(PRED_JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "#     for i, line in enumerate(f):\n",
        "#         if i >= 5:\n",
        "#             break\n",
        "#         row = json.loads(line)\n",
        "#         print(f\"[{row['example_id']}] {row['prediction'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb16b1ed",
      "metadata": {
        "id": "eb16b1ed"
      },
      "source": [
        "## 7. BLEURT-base-128 체크포인트 자동 다운로드(로컬 캐시 재사용)\n",
        "\n",
        "ToTTo 리더보드 공식 체크포인트: [BLEURT-base-128](https://github.com/google-research/bleurt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba37e21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eba37e21",
        "outputId": "e3338d74-f91e-4d1b-97af-19c12b9f386f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using existing BLEURT checkpoint: /content/drive/MyDrive/nlp_project_02/bleurt_ckpts/bleurt-base-128\n"
          ]
        }
      ],
      "source": [
        "def prepare_bleurt_base_128_checkpoint(\n",
        "    cache_dir: str, dirname: str = \"bleurt-base-128\"\n",
        ") -> str:\n",
        "    \"\"\"BLEURT-base-128 체크포인트가 cache_dir/dirname에 없으면 다운로드+압축해제\n",
        "\n",
        "    ToTTo 리더보드 공식 체크포인트: BLEURT-base-128\n",
        "    https://github.com/google-research/bleurt\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    ckpt_path = os.path.join(cache_dir, dirname)\n",
        "\n",
        "\n",
        "    # 이미 존재하면 재사용\n",
        "\n",
        "    if os.path.isdir(ckpt_path) and os.listdir(ckpt_path):\n",
        "\n",
        "        print(\"Using existing BLEURT checkpoint:\", ckpt_path)\n",
        "        return ckpt_path\n",
        "\n",
        "\n",
        "    url = \"https://storage.googleapis.com/bleurt-oss/bleurt-base-128.zip\"\n",
        "    zip_path = os.path.join(cache_dir, \"bleurt-base-128.zip\")\n",
        "\n",
        "\n",
        "    print(\"Downloading BLEURT-base-128 checkpoint (ToTTo official)...\")\n",
        "\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "\n",
        "    print(\"Unzipping...\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "\n",
        "        zf.extractall(cache_dir)\n",
        "\n",
        "\n",
        "    if not os.path.isdir(ckpt_path):\n",
        "\n",
        "        # 일부 환경에서는 대소문자/폴더명 변형 가능성 대비\n",
        "\n",
        "        # cache_dir 아래의 bleurt-base-128* 후보를 탐색\n",
        "\n",
        "        candidates = [\n",
        "            os.path.join(cache_dir, d)\n",
        "            for d in os.listdir(cache_dir)\n",
        "\n",
        "            if d.lower().startswith(\"bleurt-base-128\")\n",
        "            and os.path.isdir(os.path.join(cache_dir, d))\n",
        "        ]\n",
        "\n",
        "        if candidates:\n",
        "\n",
        "            ckpt_path = candidates[0]\n",
        "\n",
        "\n",
        "    print(\"BLEURT checkpoint prepared at:\", ckpt_path)\n",
        "    return ckpt_path\n",
        "\n",
        "\n",
        "\n",
        "bleurt_ckpt = prepare_bleurt_base_128_checkpoint(\n",
        "    BLEURT_CACHE_DIR, BLEURT_BASE_128_DIRNAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d57c2ec",
      "metadata": {
        "id": "8d57c2ec"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "class Metric:\n",
        "\n",
        "    \"\"\"(preds: List[str], refs: List[List[str]]) -> float\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    refs는 example별 multiple reference(list[str])를 받습니다.\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    name: str\n",
        "\n",
        "\n",
        "    def compute(\n",
        "        self, preds: List[str], refs: List[List[str]], batch_size: int\n",
        "    ) -> float:\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "class MetricRegistry:\n",
        "\n",
        "    def __init__(self, metrics: List[Metric], batch_size: int):\n",
        "\n",
        "        self.metrics = metrics\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "    def run(self, preds: List[str], refs: List[List[str]]) -> Dict[str, float]:\n",
        "\n",
        "        results: Dict[str, float] = {}\n",
        "\n",
        "        for m in self.metrics:\n",
        "\n",
        "            print(f\"\\n[Metric] {m.name} (batch_size={self.batch_size})\")\n",
        "\n",
        "            val = m.compute(preds, refs, batch_size=self.batch_size)\n",
        "\n",
        "            results[m.name] = float(val)\n",
        "\n",
        "            self._cleanup()\n",
        "        return results\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _cleanup():\n",
        "\n",
        "\n",
        "        # PyTorch GPU cache\n",
        "\n",
        "        try:\n",
        "\n",
        "            import torch\n",
        "\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "        # TF graph/session 정리 (BLEURT 후 메모리 회수 목적)\n",
        "\n",
        "        try:\n",
        "\n",
        "            import tensorflow as tf\n",
        "\n",
        "\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "def compute_bleu_sacrebleu(preds: List[str], multi_refs: List[List[str]]) -> float:\n",
        "\n",
        "    # sacrebleu는 reference set을 (num_refs, num_examples) 형태의 list-of-lists로 받습니다.\n",
        "\n",
        "    n_refs_each = [len(r) for r in multi_refs]\n",
        "\n",
        "    max_refs = max(n_refs_each) if n_refs_each else 0\n",
        "\n",
        "\n",
        "    if max_refs == 0:\n",
        "\n",
        "        raise ValueError(\"No references found for BLEU computation.\")\n",
        "\n",
        "\n",
        "    # reference 개수가 가변인 경우: 부족한 ref는 첫 ref로 채움(최소한의 보정)\n",
        "\n",
        "    norm_refs = []\n",
        "    for refs in multi_refs:\n",
        "\n",
        "\n",
        "        if len(refs) < max_refs:\n",
        "\n",
        "            if len(refs) == 0:\n",
        "\n",
        "                refs = [\"\"] * max_refs\n",
        "            else:\n",
        "\n",
        "\n",
        "                refs = refs + [refs[0]] * (max_refs - len(refs))\n",
        "\n",
        "        norm_refs.append(refs)\n",
        "\n",
        "\n",
        "    # transpose: [ [ref_i for ex] for i ]\n",
        "\n",
        "    ref_sets = [\n",
        "        [norm_refs[j][i] for j in range(len(norm_refs))] for i in range(max_refs)\n",
        "    ]\n",
        "\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(preds, ref_sets)\n",
        "\n",
        "    return float(bleu.score)\n",
        "\n",
        "\n",
        "\n",
        "class BleuMetric(Metric):\n",
        "    def __init__(self):\n",
        "\n",
        "\n",
        "        self.name = \"BLEU_sacrebleu\"\n",
        "\n",
        "\n",
        "    def compute(\n",
        "        self, preds: List[str], refs: List[List[str]], batch_size: int\n",
        "    ) -> float:\n",
        "\n",
        "        # BLEU는 GPU를 쓰지 않지만 registry 일관성을 위해 포함\n",
        "\n",
        "        return compute_bleu_sacrebleu(preds, refs)\n",
        "\n",
        "\n",
        "\n",
        "def compute_bleurt_avg(\n",
        "    scorer: bleurt_score.BleurtScorer,\n",
        "    preds: List[str],\n",
        "\n",
        "    multi_refs: List[List[str]],\n",
        "\n",
        "    pair_chunk_size: int = 256,\n",
        "    batch_size: int = 16,\n",
        ") -> Tuple[float, List[float]]:\n",
        "\n",
        "    \"\"\"Returns (mean_bleurt, per_example_avg_scores).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    - example당 multiple reference 점수 중 average aggregation (ToTTo 공식 방식)\n",
        "    - \"To handle multiple references, we take the average of the scores\" (Sellam et al. 2020)\n",
        "\n",
        "\n",
        "\n",
        "    - BLEURT scorer.score는 references/candidates 리스트를 받고, pair-wise 점수 리스트를 반환\n",
        "\n",
        "\n",
        "\n",
        "    - pair_chunk_size는 (cand, ref) pair 리스트를 chunking 하는 단위\n",
        "\n",
        "\n",
        "\n",
        "    - batch_size는 scorer.score 내부의 TF batch_size\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    n = len(preds)\n",
        "\n",
        "    # 각 example별 점수 누적 리스트\n",
        "    score_lists: List[List[float]] = [[] for _ in range(n)]\n",
        "\n",
        "\n",
        "    all_cands: List[str] = []\n",
        "\n",
        "    all_refs: List[str] = []\n",
        "\n",
        "    all_idx: List[int] = []\n",
        "\n",
        "\n",
        "    for i, (cand, refs) in enumerate(zip(preds, multi_refs)):\n",
        "        for r in refs:\n",
        "\n",
        "\n",
        "            all_cands.append(cand)\n",
        "\n",
        "            all_refs.append(r)\n",
        "\n",
        "            all_idx.append(i)\n",
        "\n",
        "\n",
        "    for start in tqdm(\n",
        "        range(0, len(all_cands), pair_chunk_size), desc=\"BLEURT scoring (pairs)\"\n",
        "    ):\n",
        "\n",
        "        end = min(start + pair_chunk_size, len(all_cands))\n",
        "\n",
        "        c_chunk = all_cands[start:end]\n",
        "\n",
        "        r_chunk = all_refs[start:end]\n",
        "\n",
        "        idx_chunk = all_idx[start:end]\n",
        "\n",
        "\n",
        "        scores = scorer.score(\n",
        "            references=r_chunk, candidates=c_chunk, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        for s, idx in zip(scores, idx_chunk):\n",
        "\n",
        "            score_lists[idx].append(float(s))\n",
        "\n",
        "\n",
        "    # 각 example별 average\n",
        "    avg_scores = [float(np.mean(sl)) if sl else float(\"nan\") for sl in score_lists]\n",
        "    mean_score = float(np.mean(avg_scores)) if avg_scores else float(\"nan\")\n",
        "    return mean_score, avg_scores\n",
        "\n",
        "\n",
        "\n",
        "class BleurtAvgMetric(Metric):\n",
        "\n",
        "    def __init__(self, bleurt_ckpt_path: str, pair_chunk_size: int):\n",
        "\n",
        "        self.name = \"BLEURT_base128_avg\"\n",
        "\n",
        "        self.bleurt_ckpt_path = bleurt_ckpt_path\n",
        "\n",
        "        self.pair_chunk_size = pair_chunk_size\n",
        "\n",
        "\n",
        "    def compute(\n",
        "        self, preds: List[str], refs: List[List[str]], batch_size: int\n",
        "    ) -> float:\n",
        "\n",
        "        # TF 기반 (GPU 사용)\n",
        "\n",
        "        scorer = bleurt_score.BleurtScorer(self.bleurt_ckpt_path)\n",
        "\n",
        "        mean_score, _ = compute_bleurt_avg(\n",
        "            scorer=scorer,\n",
        "            preds=preds,\n",
        "            multi_refs=refs,\n",
        "            pair_chunk_size=self.pair_chunk_size,\n",
        "\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        return float(mean_score)\n",
        "\n",
        "\n",
        "\n",
        "class BertScoreF1Metric(Metric):\n",
        "\n",
        "    def __init__(\n",
        "        self, model_type: str = \"roberta-large\", rescale_with_baseline: bool = True\n",
        "    ):\n",
        "\n",
        "        self.name = \"BERTScore_F1_mean\"\n",
        "\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.rescale_with_baseline = rescale_with_baseline\n",
        "\n",
        "\n",
        "    def compute(\n",
        "        self, preds: List[str], refs: List[List[str]], batch_size: int\n",
        "    ) -> float:\n",
        "\n",
        "        # BERTScore는 ref를 1:1로 받으므로, multiple ref인 경우 첫 ref를 사용\n",
        "\n",
        "        # (BLEURT는 max aggregation으로 multi-ref를 반영)\n",
        "\n",
        "        one_ref = [r[0] if r else \"\" for r in refs]\n",
        "\n",
        "\n",
        "        import torch\n",
        "\n",
        "        from bert_score import score as bert_score\n",
        "\n",
        "\n",
        "        P, R, F1 = bert_score(\n",
        "            cands=preds,\n",
        "            refs=one_ref,\n",
        "            model_type=self.model_type,\n",
        "\n",
        "            lang=\"en\",\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            batch_size=batch_size,\n",
        "            idf=False,\n",
        "            rescale_with_baseline=self.rescale_with_baseline,\n",
        "\n",
        "            verbose=True,\n",
        "        )\n",
        "\n",
        "\n",
        "        return float(F1.mean().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b0a182",
      "metadata": {
        "id": "b3b0a182"
      },
      "source": [
        "## 8. Metric 계산 (GPU, 순차 실행 + 캐시 정리)\n",
        "\n",
        "- BLEU: sacrebleu의 corpus BLEU (multiple reference)\n",
        "- BLEURT: example당 multiple reference 점수 계산 후 **average aggregation** (TF, GPU)\n",
        "  - ToTTo 공식: BLEURT-base-128, average aggregation (Sellam et al. 2020)\n",
        "- BERTScore: roberta-large, rescale_with_baseline=True (PyTorch, GPU)\n",
        "\n",
        "> 참고: TF(BLEURT) + PyTorch(BERTScore)를 단일 프로세스에서 GPU로 사용하므로, metric은 **순차 실행**하며 실행 사이에 GPU 캐시를 정리합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97332471",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389,
          "referenced_widgets": [
            "a8787700b98040fd8e03d0a5499ecdc7",
            "71f56a65417641fe854f5e3b6f9811c1",
            "5151e35da8a94ebb8d5fd3f7b5bd69fd",
            "dce68745c4834280b64601bac79ca5f3",
            "8e64296e7a3445aa9b3560ac892fbdf1",
            "1c27be65a20741da9103ef3ca817b7b6",
            "ea54032a85e4420ca6606b1dba463070",
            "cf45f4e320bb4475881a9fe2eb689459",
            "7257a1cec59d413988d0a303131c7c8d",
            "228d8b85c50745698692eed86c7306eb",
            "a02e2d1e6e7649a9bcb4ac3a1e7a4a89",
            "bb9368d4c7184009b5798ab38c5fac15",
            "8063f9c3f9794cd8a3ec4daa453fba8d",
            "9658fa928fbd48b18e7bf108b0e7407f",
            "d26a579108ee4c9da703aca27ccf089c",
            "801b7e22f1ad4c75a423630d35cbf634",
            "669a0c212cea491ca35e01afdb6aa121",
            "9729103bd4d84a43abb67df3ba029fcf",
            "c3f3d3d75ff2458fafa77f13b07d4218",
            "af13fe7bdd8e4c49b347ba2d74b0e0b9",
            "0b1a27a4c79144cabdef0660f23293c1",
            "cd09bf441c7a43f9b9906525bb3806d1"
          ]
        },
        "id": "97332471",
        "outputId": "39c34a48-37b9-4bb9-f051-440868c48d01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Metric] BLEU_sacrebleu (batch_size=16)\n",
            "\n",
            "[Metric] BLEURT_base128_avg (batch_size=16)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BLEURT scoring (pairs): 100%|██████████| 88/88 [00:28<00:00,  3.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Metric] BERTScore_F1_mean (batch_size=16)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8787700b98040fd8e03d0a5499ecdc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/950 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb9368d4c7184009b5798ab38c5fac15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1394 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 33.97 seconds, 656.17 sentences/sec\n",
            "Saved metrics to: /content/drive/MyDrive/nlp_project_02/preds/totto_test_metrics.json\n",
            "{'num_examples_scored': 22293, 'num_examples_dropped_missing_refs': 0, 'BLEU_sacrebleu': 30.448489068209962, 'BLEURT_base128_avg': 0.1311773458690498, 'BERTScore_F1_mean': 0.5829211473464966}\n"
          ]
        }
      ],
      "source": [
        "def load_pred_jsonl(pred_jsonl_path: str) -> List[Dict]:\n",
        "    rows = []\n",
        "    with open(pred_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "\n",
        "if RUN_SCORING:\n",
        "    rows = load_pred_jsonl(PRED_JSONL_PATH)\n",
        "\n",
        "    # references 없는 샘플은 제외(또는 에러 처리 가능)\n",
        "    filtered = [r for r in rows if r.get(\"references\")]\n",
        "    dropped = len(rows) - len(filtered)\n",
        "    if dropped:\n",
        "        print(f\"Warning: dropped {dropped} examples because references were missing.\")\n",
        "\n",
        "    preds = [str(r[\"prediction\"]) for r in filtered]\n",
        "    refs = [r[\"references\"] for r in filtered]\n",
        "\n",
        "    # ---- Metric Registry (순차 실행 + 캐시 정리) ----\n",
        "    # ToTTo 공식: BLEURT-base-128, average aggregation\n",
        "    registry = MetricRegistry(\n",
        "        metrics=[\n",
        "            BleuMetric(),\n",
        "            BleurtAvgMetric(\n",
        "                bleurt_ckpt_path=bleurt_ckpt, pair_chunk_size=PAIR_CHUNK_SIZE\n",
        "            ),\n",
        "            BertScoreF1Metric(model_type=\"roberta-large\", rescale_with_baseline=True),\n",
        "        ],\n",
        "        batch_size=EVAL_BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "    results = registry.run(preds, refs)\n",
        "\n",
        "    # ---- Save metrics ----\n",
        "    ensure_dir(METRICS_JSON_PATH)\n",
        "    metrics = {\n",
        "        \"num_examples_scored\": len(filtered),\n",
        "        \"num_examples_dropped_missing_refs\": dropped,\n",
        "        **results,\n",
        "    }\n",
        "\n",
        "    with open(METRICS_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"Saved metrics to:\", METRICS_JSON_PATH)\n",
        "    print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076a4597",
      "metadata": {
        "id": "076a4597"
      },
      "source": [
        "## 9. (선택) Error Analysis 힌트\n",
        "\n",
        "- `PRED_JSONL_PATH`를 기반으로 예측이 낮게 나오는 샘플을 추출하고, input/refs/pred를 함께 확인하면 디버깅이 수월합니다.\n",
        "- ToTTo는 Table grounding 성격이 강하므로, BLEURT/BLEU 외에 PARENT 같은 테이블 기반 metric도 병행하면 분석이 더 탄탄해집니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ugQqWhyDX5MM",
      "metadata": {
        "id": "ugQqWhyDX5MM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b1a27a4c79144cabdef0660f23293c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c27be65a20741da9103ef3ca817b7b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "228d8b85c50745698692eed86c7306eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5151e35da8a94ebb8d5fd3f7b5bd69fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf45f4e320bb4475881a9fe2eb689459",
            "max": 950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7257a1cec59d413988d0a303131c7c8d",
            "value": 950
          }
        },
        "669a0c212cea491ca35e01afdb6aa121": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f56a65417641fe854f5e3b6f9811c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c27be65a20741da9103ef3ca817b7b6",
            "placeholder": "​",
            "style": "IPY_MODEL_ea54032a85e4420ca6606b1dba463070",
            "value": "100%"
          }
        },
        "7257a1cec59d413988d0a303131c7c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "801b7e22f1ad4c75a423630d35cbf634": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8063f9c3f9794cd8a3ec4daa453fba8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_669a0c212cea491ca35e01afdb6aa121",
            "placeholder": "​",
            "style": "IPY_MODEL_9729103bd4d84a43abb67df3ba029fcf",
            "value": "100%"
          }
        },
        "8e64296e7a3445aa9b3560ac892fbdf1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9658fa928fbd48b18e7bf108b0e7407f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3f3d3d75ff2458fafa77f13b07d4218",
            "max": 1394,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af13fe7bdd8e4c49b347ba2d74b0e0b9",
            "value": 1394
          }
        },
        "9729103bd4d84a43abb67df3ba029fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a02e2d1e6e7649a9bcb4ac3a1e7a4a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8787700b98040fd8e03d0a5499ecdc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71f56a65417641fe854f5e3b6f9811c1",
              "IPY_MODEL_5151e35da8a94ebb8d5fd3f7b5bd69fd",
              "IPY_MODEL_dce68745c4834280b64601bac79ca5f3"
            ],
            "layout": "IPY_MODEL_8e64296e7a3445aa9b3560ac892fbdf1"
          }
        },
        "af13fe7bdd8e4c49b347ba2d74b0e0b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb9368d4c7184009b5798ab38c5fac15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8063f9c3f9794cd8a3ec4daa453fba8d",
              "IPY_MODEL_9658fa928fbd48b18e7bf108b0e7407f",
              "IPY_MODEL_d26a579108ee4c9da703aca27ccf089c"
            ],
            "layout": "IPY_MODEL_801b7e22f1ad4c75a423630d35cbf634"
          }
        },
        "c3f3d3d75ff2458fafa77f13b07d4218": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd09bf441c7a43f9b9906525bb3806d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf45f4e320bb4475881a9fe2eb689459": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d26a579108ee4c9da703aca27ccf089c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b1a27a4c79144cabdef0660f23293c1",
            "placeholder": "​",
            "style": "IPY_MODEL_cd09bf441c7a43f9b9906525bb3806d1",
            "value": " 1394/1394 [00:05&lt;00:00, 235.04it/s]"
          }
        },
        "dce68745c4834280b64601bac79ca5f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_228d8b85c50745698692eed86c7306eb",
            "placeholder": "​",
            "style": "IPY_MODEL_a02e2d1e6e7649a9bcb4ac3a1e7a4a89",
            "value": " 950/950 [00:27&lt;00:00, 51.25it/s]"
          }
        },
        "ea54032a85e4420ca6606b1dba463070": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}